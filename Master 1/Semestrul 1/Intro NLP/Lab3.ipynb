{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZ0vy1SifWLS"
   },
   "source": [
    "#Word Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd2CqfL-f1nU"
   },
   "source": [
    "###Bag of Words\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. \n",
    "\n",
    "It is called a ***bag*** of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
    "\n",
    "If your dataset is small and context is domain specific, BoW may work better than Word Embedding because you may not find the corresponding vector from pre-trained word embedding models for some of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OkGW5V20vpo"
   },
   "source": [
    "![bow](https://miro.medium.com/max/554/0*B9GC_f3BMtjGMdQ-.png)\n",
    "\n",
    "[Photo source](https://medium.com/analytics-vidhya/does-tf-idf-work-differently-in-textbooks-and-sklearn-routine-cc7a7d1b580d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixAb90cV2smS",
    "outputId": "f25d3634-fbc6-4b93-9dcb-3554ea847114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Flora is all the plant life present in a particular region or time, generally the naturally occurring (indigenous) native plants.',\n",
       " 'The corresponding term for animal life is fauna. Flora, fauna, and other forms of life, such as fungi, are collectively referred to as biota.',\n",
       " 'Sometimes bacteria and fungi are also referred to as flora, as in the terms gut flora or skin flora.']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"Flora is all the plant life present in a particular region or time, generally the naturally occurring (indigenous) native plants.\",\n",
    "    \"The corresponding term for animal life is fauna. Flora, fauna, and other forms of life, such as fungi, are collectively referred to as biota.\",\n",
    "    \"Sometimes bacteria and fungi are also referred to as flora, as in the terms gut flora or skin flora.\"]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thMH00eL4WC1"
   },
   "source": [
    "We will create the BoW vectors using `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jnfTYi2U3qi6",
    "outputId": "e3334d72-ba7a-49d4-b7aa-85825bc75c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Flora', 'Sometimes', 'The', 'all', 'also', 'and', 'animal', 'are', 'as', 'bacteria', 'biota', 'collectively', 'corresponding', 'fauna', 'flora', 'for', 'forms', 'fungi', 'generally', 'gut', 'in', 'indigenous', 'is', 'life', 'native', 'naturally', 'occurring', 'of', 'or', 'other', 'particular', 'plant', 'plants', 'present', 'referred', 'region', 'skin', 'such', 'term', 'terms', 'the', 'time', 'to']\n",
      "43\n",
      "[[1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1\n",
      "  0 0 0 0 2 1 0]\n",
      " [1 0 1 0 0 1 1 1 2 0 1 1 1 2 0 1 1 1 0 0 0 0 1 2 0 0 0 1 0 1 0 0 0 0 1 0\n",
      "  0 1 1 0 0 0 1]\n",
      " [0 1 0 0 1 1 0 1 2 1 0 0 0 0 3 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      "  1 0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase = False)\n",
    "bow_representation = vectorizer.fit_transform(corpus)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "print(vocabulary)\n",
    "print(len(vocabulary))\n",
    "print(bow_representation.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AF5RZAfAJoRl"
   },
   "source": [
    "####N-grams encoding\n",
    "\n",
    "Extracts features from text while capturing local word order by defining\n",
    "counts over sliding windows.\n",
    "\n",
    "![ngrams](https://i.stack.imgur.com/8ARA1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IhluFw_KceT",
    "outputId": "a9419f0d-1555-43ff-fc8a-1beee53e1ec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Flora fauna', 'Flora is', 'Sometimes bacteria', 'The corresponding', 'all the', 'also referred', 'and fungi', 'and other', 'animal life', 'are also', 'are collectively', 'as biota', 'as flora', 'as fungi', 'as in', 'bacteria and', 'collectively referred', 'corresponding term', 'fauna Flora', 'fauna and', 'flora as', 'flora or', 'for animal', 'forms of', 'fungi are', 'generally the', 'gut flora', 'in particular', 'in the', 'indigenous native', 'is all', 'is fauna', 'life is', 'life present', 'life such', 'native plants', 'naturally occurring', 'occurring indigenous', 'of life', 'or skin', 'or time', 'other forms', 'particular region', 'plant life', 'present in', 'referred to', 'region or', 'skin flora', 'such as', 'term for', 'terms gut', 'the naturally', 'the plant', 'the terms', 'time generally', 'to as']\n",
      "56\n",
      "[[0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1\n",
      "  1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0\n",
      "  0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "bigram = CountVectorizer(lowercase = False, ngram_range=(2, 2))\n",
    "bigram_representation = bigram.fit_transform(corpus)\n",
    "\n",
    "bigram_vocabulary = bigram.get_feature_names()\n",
    "\n",
    "print(bigram_vocabulary)\n",
    "print(len(bigram_vocabulary))\n",
    "print(bigram_representation.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vLypvTLff-H"
   },
   "source": [
    "###TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOMJQfEQTE74"
   },
   "source": [
    "TF-IDF represents\n",
    "text data by indicating the importance of the word relative to the other words in\n",
    "the text.\n",
    "\n",
    "A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content” to the model as rarer but perhaps domain specific words.\n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear in all documents, so that the scores for frequent words like “the” that are also frequent across all documents are penalized.\n",
    "\n",
    "This approach to scoring is called Term Frequency – Inverse Document Frequency, or TF-IDF for short, where:\n",
    "\n",
    "- **Term Frequency**: the frequency of a given term in a document.\n",
    "\n",
    "\n",
    "\n",
    "- **Inverse Document Frequency**: the ratio of documents that contain a given term.\n",
    "\n",
    "![tf](https://www.affde.com/uploads/article/5516/PVpklt43xBCKRFBa.png)\n",
    "\n",
    "TF-IDF penalizes stopwords, they will not have a high score, but stopwords removal may stil be used to reduce the dimensionality of the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uALv5QN0Swxc",
    "outputId": "8c6b7da0-4f34-4503-e9d4-63a50a14a7c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18334923 0.         0.         0.2410822  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2410822  0.         0.18334923 0.2410822  0.18334923 0.18334923\n",
      "  0.2410822  0.2410822  0.2410822  0.         0.18334923 0.\n",
      "  0.2410822  0.2410822  0.2410822  0.2410822  0.         0.2410822\n",
      "  0.         0.         0.         0.         0.36669846 0.2410822\n",
      "  0.        ]\n",
      " [0.15630031 0.         0.20551613 0.         0.         0.15630031\n",
      "  0.20551613 0.15630031 0.31260063 0.         0.20551613 0.20551613\n",
      "  0.20551613 0.41103226 0.         0.20551613 0.20551613 0.15630031\n",
      "  0.         0.         0.         0.         0.15630031 0.31260063\n",
      "  0.         0.         0.         0.20551613 0.         0.20551613\n",
      "  0.         0.         0.         0.         0.15630031 0.\n",
      "  0.         0.20551613 0.20551613 0.         0.         0.\n",
      "  0.15630031]\n",
      " [0.         0.21348818 0.         0.         0.21348818 0.16236326\n",
      "  0.         0.16236326 0.32472653 0.21348818 0.         0.\n",
      "  0.         0.         0.64046454 0.         0.         0.16236326\n",
      "  0.         0.21348818 0.16236326 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16236326 0.\n",
      "  0.         0.         0.         0.         0.16236326 0.\n",
      "  0.21348818 0.         0.         0.21348818 0.16236326 0.\n",
      "  0.16236326]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase = False)\n",
    "tfidf_representation = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_representation.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBRqxWPySXvI"
   },
   "source": [
    "####Limitations\n",
    "\n",
    "- **Vocabulary**: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\n",
    "\n",
    "- **Sparsity:** Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.\n",
    "\n",
    "- **Meaning**: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (“this is interesting” vs “is this interesting”), synonyms (“old bike” vs “used bike”), and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8pLENc-usC-"
   },
   "source": [
    "##Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJnbhFjtzEtu",
    "outputId": "e8e890f1-d25d-41a1-f272-d46db5cd5685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85ltO6u9bbWk",
    "outputId": "4ff9c70d-fe21-45f7-f39b-8820a9e29cdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus  import twitter_samples\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "print(len(pos_tweets))\n",
    "\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print(len(neg_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPa_1MLnccON"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pos_df = pd.DataFrame(pos_tweets, columns = ['tweet'])\n",
    "pos_df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef0LpGyddQ5H"
   },
   "outputs": [],
   "source": [
    "neg_df = pd.DataFrame(neg_tweets, columns = ['tweet'])\n",
    "neg_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "-dAAOa9WdalL",
    "outputId": "a8793a2c-2116-4872-d395-e947d0d40e4e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I wanna change my avi but uSanele :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>MY PUPPY BROKE HER FOOT :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>where's all the jaebum baby pictures :((</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>But but Mr Ahmad Maslan cooks too :( https://t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>@eawoman As a Hull supporter I am expecting a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  label\n",
       "0     #FollowFriday @France_Inte @PKuchly57 @Milipol...      1\n",
       "1     @Lamb2ja Hey James! How odd :/ Please call our...      1\n",
       "2     @DespiteOfficial we had a listen last night :)...      1\n",
       "3                                  @97sides CONGRATS :)      1\n",
       "4     yeaaaah yippppy!!!  my accnt verified rqst has...      1\n",
       "...                                                 ...    ...\n",
       "9995               I wanna change my avi but uSanele :(      0\n",
       "9996                         MY PUPPY BROKE HER FOOT :(      0\n",
       "9997           where's all the jaebum baby pictures :((      0\n",
       "9998  But but Mr Ahmad Maslan cooks too :( https://t...      0\n",
       "9999  @eawoman As a Hull supporter I am expecting a ...      0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiLCqzVtvC18"
   },
   "source": [
    "####Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhrzAx8ZduUU",
    "outputId": "5fd49fa9-7dbe-4453-841c-277b0deab8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweet  label\n",
      "8693  @cIaricestarling i know right... :( i hope it'...      0\n",
      "8098  Struggling like crazy to get into a race fan m...      0\n",
      "9132                   7pm on a Friday and I am dead :(      0\n",
      "4439  @AsianMeerkat @johncrossmirror far from it. Be...      1\n",
      "665   @FooWhiter Ugh. I've never Rt or fade any of y...      1\n",
      "...                                                 ...    ...\n",
      "8671  @bumkeyyfel clowns? i'm not scared of clowns t...      0\n",
      "3441  @SachinKalbag Ah Sachin, why do you bring up u...      1\n",
      "9915  @annnalucz i'm not going :( kailan ba? may tix...      0\n",
      "6466                     @horan_lyra done, please me :(      0\n",
      "8630  I'm craving breakfast food so badly right now ...      0\n",
      "\n",
      "[8000 rows x 2 columns]\n",
      "                                                  tweet  label\n",
      "3688                             @btsmaqnae followed :)      1\n",
      "671               @4eyedmonk awesome :) I'll be waiting      1\n",
      "942   Thanks for connecting @garrowlscq Hope you're ...      1\n",
      "613    @cotterw @urihoresh we make it better though :-)      1\n",
      "3618  @monolifemusic @DJANORAK I'm getting there! It...      1\n",
      "...                                                 ...    ...\n",
      "5855  Snapchat me : LisaHerring19 #snapchat #kikme #...      0\n",
      "1401           Worth It - Fifth Harmony, don't judge :)      1\n",
      "6862                @taeserasera OMG! What happened? :(      0\n",
      "9644                       @fvvxkk that's a damn lie :(      0\n",
      "524        The long wait is over :)\\n#OTWOLGrandTrailer      1\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2, shuffle = True)\n",
    "print(train_df)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hENqcGGeBjx"
   },
   "source": [
    "####TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXgh5E4PeEdx"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase = False)\n",
    "tfidf_representation = tfidf_vectorizer.fit(train_df['tweet'])\n",
    "\n",
    "X_train = tfidf_vectorizer.transform(train_df['tweet'])\n",
    "X_test = tfidf_vectorizer.transform(test_df['tweet'])\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMMWmLnegV50"
   },
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SleRBR3Gfcx2",
    "outputId": "6119685b-95c4-42a4-f90d-23446113a1ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaRmuGBPfqH8",
    "outputId": "0ddb97b4-bc37-47b0-853f-7c91b7e0957e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score 0.897125\n",
      "Test Score 0.7635\n"
     ]
    }
   ],
   "source": [
    "print('Train Score', logreg.score(X_train, y_train))\n",
    "print('Test Score', logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IAsQmZ9Q5J_"
   },
   "source": [
    "##Assignment\n",
    "\n",
    "To be uploaded here: https://forms.gle/qTzLy6F6jkUtQrvy7 until November 17th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj_ckmB_Q8JS"
   },
   "source": [
    "Investigate the effect of text normalization.\n",
    "\n",
    "- Search for a dataset for classification (or experiment with the same dataset from this lab)\n",
    "- Preprocess the text\n",
    "- Compare the vocabulary size with and without preprocessing\n",
    "- Get the numerical representation of the text\n",
    "- Train a model\n",
    "- Test your model \n",
    "- Compare the performance of your model with and without text normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-iBDtB2zWu5"
   },
   "source": [
    "##Pick database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APxISn--yb0U",
    "outputId": "502034fb-9285-41f2-f49a-1f4f5c43dc73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "IOonTxZXyhRL"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus  import twitter_samples\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JoU7ZPq0_bS"
   },
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuA965N8Jn0J",
    "outputId": "5c4d442e-7781-4df1-9ff1-2c62599366fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 5.2 MB/s \n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
      "\u001b[K     |████████████████████████████████| 628 kB 57.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 59.1 MB/s \n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
      "\u001b[K     |████████████████████████████████| 451 kB 39.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 43.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 2.2.4\n",
      "    Uninstalling spacy-2.2.4:\n",
      "      Successfully uninstalled spacy-2.2.4\n",
      "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.0 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 96 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.6.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.5\n",
      "    Uninstalling en-core-web-sm-2.2.5:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.5\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odxiq5LA4_Hg",
    "outputId": "946580ff-3f14-4f77-ece4-c883e9aae3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "k9T6le2U4kIc"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words_nltk = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "ZDW_I3zjyN7i"
   },
   "outputs": [],
   "source": [
    "def preprocess(tweets):\n",
    "  # it makes sense to keep hashtags, punctuation, emoticons and emojis\n",
    "  prepocessed_tweets = []\n",
    "\n",
    "  for text in tweets:\n",
    "    # lowercase text, remove newline characters, mentions, links and digits\n",
    "    text = ' '.join([word.lower() for word in text.split() \n",
    "                    if word[0] not in ['@'] and word[:4] != \"http\" \n",
    "                    and not word.isdigit()])\n",
    "\n",
    "    # tokenize into separate words, remove stopwords, lemmatize\n",
    "    text = ' '.join([word.lemma_ for word in nlp(text) if word not in stop_words_nltk])\n",
    "\n",
    "    prepocessed_tweets += [text]\n",
    "\n",
    "  return prepocessed_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiULotN51Jqh"
   },
   "source": [
    "##Compare the vocabulary size with and without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "8Fc1mKm47X7I"
   },
   "outputs": [],
   "source": [
    "def getVocabularySize(tweets):\n",
    "  vocabulary = set()\n",
    "  for tweet in tweets:\n",
    "    for word in tweet:\n",
    "      vocabulary.add(word)\n",
    "  return len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "NmH7r0RfyS0r"
   },
   "outputs": [],
   "source": [
    "def printTable():\n",
    "  print('             | Positive Tweets | Negative Tweets')\n",
    "  print('             | Vocabulary      | Vocabulary     ')\n",
    "  print('------------------------------------------------')\n",
    "  print('Original     | ' + \n",
    "        str(getVocabularySize(positive_tweets)) + \n",
    "        '             | ' + \n",
    "        str(getVocabularySize(negative_tweets)))\n",
    "  print('------------------------------------------------')\n",
    "  print('Preprocessed | ' + \n",
    "        str(getVocabularySize(preprocessed_positive_tweets)) + \n",
    "        '             | ' + \n",
    "        str(getVocabularySize(preprocessed_negative_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "DZmSy7sq-nbA"
   },
   "outputs": [],
   "source": [
    "preprocessed_positive_tweets = preprocess(positive_tweets)\n",
    "preprocessed_negative_tweets = preprocess(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QTaTLjPBWKh",
    "outputId": "d30b8c3b-161d-4eae-f236-fe880401511f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             | Positive Tweets | Negative Tweets\n",
      "             | Vocabulary      | Vocabulary     \n",
      "------------------------------------------------\n",
      "Original     | 274             | 248\n",
      "------------------------------------------------\n",
      "Preprocessed | 244             | 217\n"
     ]
    }
   ],
   "source": [
    "printTable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gln7ASWW2wwv"
   },
   "source": [
    "##Get numerical representation of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "dY_7w1yN-UgI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "q8s0iot1-Kv-"
   },
   "outputs": [],
   "source": [
    "def getDataFrame(positive_data, negative_data):\n",
    "  pos_df = pd.DataFrame(positive_data, columns = ['tweet'])\n",
    "  pos_df['label'] = 1\n",
    "\n",
    "  neg_df = pd.DataFrame(negative_data, columns = ['tweet'])\n",
    "  neg_df['label'] = 0\n",
    "\n",
    "  data_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "  return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "_u375dAk6Rob"
   },
   "outputs": [],
   "source": [
    "original_df = getDataFrame(positive_tweets, negative_tweets)\n",
    "preprocessed_df = getDataFrame(preprocessed_positive_tweets, preprocessed_negative_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7wh4ePk23qv"
   },
   "source": [
    "##Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "aLh4odK3CeOw"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "dhD-yyQV_uJv"
   },
   "outputs": [],
   "source": [
    "def splitTrainTest(df):\n",
    "  return train_test_split(df, test_size=0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "e-Pn579ICGeA"
   },
   "outputs": [],
   "source": [
    "def train(train_df):\n",
    "  tfidf_representation = tfidf_vectorizer.fit(train_df['tweet'])\n",
    "\n",
    "  X_train = tfidf_vectorizer.transform(train_df['tweet'])\n",
    "  y_train = train_df['label']\n",
    "\n",
    "  return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "HDf1AdWhC9My"
   },
   "outputs": [],
   "source": [
    "def test(test_df):\n",
    "  X_test = tfidf_vectorizer.transform(test_df['tweet'])\n",
    "  y_test = test_df['label']\n",
    "\n",
    "  return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "bXuIh7FD_FDv"
   },
   "outputs": [],
   "source": [
    "original_train, original_test = splitTrainTest(original_df)\n",
    "preprocessed_train, preprocessed_test = splitTrainTest(preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyP0Zc3w2_Yd"
   },
   "source": [
    "##Compare performance with and without text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "rMe0a1QnDIPz"
   },
   "outputs": [],
   "source": [
    "def getScores(train_df, test_df):\n",
    "  X_train, y_train = train(train_df)\n",
    "  X_test, y_test = test(test_df)\n",
    "\n",
    "  logreg = LogisticRegression()\n",
    "  logreg.fit(X_train, y_train)\n",
    "\n",
    "  print('Train Score', logreg.score(X_train, y_train))\n",
    "  print('Test Score', logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qZVNL2zEKuF",
    "outputId": "45c92d6d-88d1-4fdc-b3cc-d967b6f921d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score 0.9015\n",
      "Test Score 0.7575\n",
      "Train Score 0.872625\n",
      "Test Score 0.767\n"
     ]
    }
   ],
   "source": [
    "getScores(original_train, original_test)\n",
    "getScores(preprocessed_train, preprocessed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__AIRTKuqbaQ"
   },
   "source": [
    "####Further reading\n",
    "\n",
    "- [TF-IDF/Term Frequency](https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3)\n",
    "- [Bag of Words](https://www.mygreatlearning.com/blog/bag-of-words/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
