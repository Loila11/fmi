{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1bls46SNnk0"
   },
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Lab overview:\n",
    "\n",
    "\n",
    "* Normalization\n",
    "* Tokenization\n",
    "* Lematization\n",
    "* Stemming\n",
    "* Stopwords removal\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nK8jcjSNkv3"
   },
   "source": [
    "##Text normalization (cleaning)\n",
    "\n",
    "Depending on the task you are cleaning the text for, you may perform one or more of: \n",
    "\n",
    "* Transform text to lowercase\n",
    "* Remove emoticons ( :) :D) and emojis (üíô üê±)\n",
    "* Remove punctuation\n",
    "* Remove digits or transform them to words\n",
    "* Correct spelling errors\n",
    "\n",
    "\n",
    "Python Regular Expressions \n",
    "*   [re Python documentation](https://docs.python.org/3/library/re.html)\n",
    "*   [Quick reference](https://www.computerhope.com/unix/regex-quickref.htm)\n",
    "*   [Cheat Sheet](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf)\n",
    "\n",
    "![regular_expressions](https://res.cloudinary.com/practicaldev/image/fetch/s--_iE0KvdT--/c_imagga_scale,f_auto,fl_progressive,h_900,q_auto,w_1600/https://dev-to-uploads.s3.amazonaws.com/i/zpek00ubevoxvn458b01.png)\n",
    "\n",
    "[Photo source](https://dev.to/mconner89/regular-expressions-grouping-and-string-methods-3ijn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChzKaNrWZ16E"
   },
   "source": [
    "Here is our text sample, a short review of the movie [Jaws](https://en.wikipedia.org/wiki/Jaws_(film))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "mwbmESJGWAre",
    "outputId": "287e85c9-e1d4-4033-e636-a57fa3059032"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\" Jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. The movie opens with blackness, and only distant, alien-like underwater sounds. :) :D It deserves 5 stars, not 4 stars.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '\" Jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. The movie opens with blackness, and only distant, alien-like underwater sounds. :) :D It deserves 5 stars, not 4 stars.'\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV-zwIaLJ_gI"
   },
   "source": [
    "Transform text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "JEhAwxLyaX7L",
    "outputId": "e7fece97-5571-48d6-dcec-a5effc2258f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves 5 stars, not 4 stars.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htuAG8zKarGo"
   },
   "source": [
    "importing [re](https://docs.python.org/3/library/re.html) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bOeMPOhga09l"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4ioMoqMP4sW"
   },
   "source": [
    "Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "1PM0guG_P60M",
    "outputId": "56ffa5c4-8176-4498-9136-4529b3daa131"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves stars, not stars.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(' \\d+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeQEFmcfRPYc"
   },
   "source": [
    "Converting numbers to words using [num2words](https://github.com/savoirfairelinux/num2words) (it works on multiple languages)\n",
    "\n",
    "We need to install the num2words library first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Vx3KhgnRPfv",
    "outputId": "17bba191-7daa-4e82-f39a-8b98fd48da92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting num2words\n",
      "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñé                            | 10 kB 21.2 MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 20 kB 24.3 MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 30 kB 11.2 MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 40 kB 9.0 MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 51 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 61 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 71 kB 5.7 MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 81 kB 6.4 MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 92 kB 4.7 MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101 kB 3.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
      "Installing collected packages: num2words\n",
      "Successfully installed num2words-0.5.10\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1K45N29RlOX"
   },
   "source": [
    "After installing, we can import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "9Egr4m_9Rl9b",
    "outputId": "9f27c5f0-9067-4186-a6c1-2d85b7912733"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves five stars, not four stars.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from num2words import num2words\n",
    "\n",
    "text = ' '.join([num2words(word) if word.isdigit() else word for word in text.split()])\n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbEeAr6pKFj0"
   },
   "source": [
    "Remove emoticons ( :) :D) and emojis (üíô üê±)\n",
    "\n",
    "Using [emoji](https://github.com/carpedm20/emoji) library or the corresponding unicode characters.\n",
    "\n",
    "We need to install the emoji library first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tq72wdvEKI7z",
    "outputId": "fa49d1f9-99ff-444e-8b1d-1809f343a3ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVu11QZEKoSW"
   },
   "source": [
    "After installing, we can import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "4HYmbrutKnEo",
    "outputId": "2265ecb0-cb27-4432-86ab-e637e92d17b8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves five stars, not four stars.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "emoji.get_emoji_regexp().sub(u'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N69dNffKLtqu"
   },
   "source": [
    "The *get_emoji_regexp()* function returns a regex to match any emoji.\n",
    "\n",
    "Another way of removing emojis with regex:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "vHSh8NUIM_bY",
    "outputId": "896b5034-8e02-4b46-ae1b-88bbf197384a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves five stars, not four stars.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    u\"\\U0001f926-\\U0001f937\"\n",
    "    u\"\\U00010000-\\U0010ffff\"\n",
    "    u\"\\u2640-\\u2642\" \n",
    "    u\"\\u2600-\\u2B55\"\n",
    "    u\"\\u200d\"\n",
    "    u\"\\u23cf\"\n",
    "    u\"\\u23e9\"\n",
    "    u\"\\u231a\"\n",
    "    u\"\\ufe0f\"\n",
    "    u\"\\u3030\"\n",
    "    \"]+\", re.UNICODE)\n",
    "\n",
    "text = re.sub(emoj, '', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_xacsPCOPjL"
   },
   "source": [
    "Removing emoticons (regex from [nltk Twitter Tokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "QeYkCRPNOS3n",
    "outputId": "ea3297f7-d9fe-4d69-a174-54ce8f392c74"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds.   it deserves five stars, not four stars.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "      |\n",
    "      </?3                       # heart\n",
    "    )\"\"\"\n",
    "    \n",
    "emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n",
    "text = re.sub(emoticon_re, '', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VkbsLqXMyYo"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "\n",
    "*   Word level: Split by whitespace, [nltk.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n",
    "*   Sentence level: Split by punctuation, [nltk.sent_tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrN047ULHZQ_",
    "outputId": "8745e44f-99fe-45cf-d8ea-0d03eb92fe08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'jaws', '\"', 'is', 'a', 'rare', 'film', 'that', 'grabs', 'your', 'attention', 'before', 'it', 'shows', 'you', 'a', 'single', 'image', 'on', 'screen.', 'the', 'movie', 'opens', 'with', 'blackness,', 'and', 'only', 'distant,', 'alien-like', 'underwater', 'sounds.', 'it', 'deserves', 'five', 'stars,', 'not', 'four', 'stars.']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhyTVP55VRwT"
   },
   "source": [
    "We need to download first the Punkt Tokenizer Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHvjfBzxVdAv",
    "outputId": "bcee1803-c83f-45f7-cdaf-6e2319284bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nZasrqFVE-h",
    "outputId": "6c691a19-a502-4874-e4d5-e86e1fe1f440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'jaws', '``', 'is', 'a', 'rare', 'film', 'that', 'grabs', 'your', 'attention', 'before', 'it', 'shows', 'you', 'a', 'single', 'image', 'on', 'screen', '.', 'the', 'movie', 'opens', 'with', 'blackness', ',', 'and', 'only', 'distant', ',', 'alien-like', 'underwater', 'sounds', '.', 'it', 'deserves', 'five', 'stars', ',', 'not', 'four', 'stars', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokenized_text_nltk = word_tokenize(text)\n",
    "print(tokenized_text_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yypjBlMVpnG"
   },
   "source": [
    "Sentence tokenization using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqqArIwwVp1y",
    "outputId": "be0b5c8e-6722-46ba-9daa-9d625a71d2ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen.',\n",
       " 'the movie opens with blackness, and only distant, alien-like underwater sounds.',\n",
       " 'it deserves five stars, not four stars.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " re.split('(?<=[.!?]) +', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7KKoyA1WNVo"
   },
   "source": [
    "Sentence tokenization using nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3RfpgBzWSaL",
    "outputId": "af5582ad-b486-46cc-b0e9-06d4de717927"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen.',\n",
       " 'the movie opens with blackness, and only distant, alien-like underwater sounds.',\n",
       " 'it deserves five stars, not four stars.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kM9idFgSWnJp",
    "outputId": "73bf0e9a-9354-403b-9409-80359687dc8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was good.Thanks.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = 'I was good.Thanks.'\n",
    "re.split('(?<=[.!?]) +', text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0DKB0VoWr8M",
    "outputId": "bcdb1572-f449-4a33-8a53-8fe5440360d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was good.Thanks.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1s4GXfHXCOl"
   },
   "source": [
    "Removing punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "zCxHclC0XF8e",
    "outputId": "1ea27390-5b23-403f-d3af-10659cea4d17"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' jaws   is a rare film that grabs your attention before it shows you a single image on screen the movie opens with blackness and only distant alienlike underwater sounds   it deserves five stars not four stars'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^\\w\\s]','', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYxNlaSgi6gH"
   },
   "source": [
    "Using [string](https://docs.python.org/3/library/string.html) library. \n",
    "\n",
    "The string.punctuation method returns a list of punctuation marks. \n",
    "\n",
    "We use the translate() method which replaces every instance of a punctuation mark with the value '' in our strings. We use the str.maketrans() method to support the translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Jid14LagYAPQ",
    "outputId": "b988262f-dd59-469d-9d9a-827f82b8c838"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' jaws   is a rare film that grabs your attention before it shows you a single image on screen the movie opens with blackness and only distant alienlike underwater sounds   it deserves five stars not four stars'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOkRYWmakLla"
   },
   "source": [
    "Removing multiple spaces between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "LHAFnlSzYu7_",
    "outputId": "dba865b0-912e-41a1-bafc-de603b671619"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' jaws is a rare film that grabs your attention before it shows you a single image on screen the movie opens with blackness and only distant alienlike underwater sounds it deserves five stars not four stars'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub(' +', ' ', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F0pEMhIHbd6"
   },
   "source": [
    "## Removing stopwords\n",
    "\n",
    "![stopwords.jpg](https://user.oc-static.com/upload/2021/01/06/16099626487943_P1C2.png) \n",
    "\n",
    "[Photo source](https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/6980726-remove-stop-words-from-a-block-of-text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E53_oLK7NP8y"
   },
   "source": [
    "###Why do we Need to Remove Stopwords?\n",
    "\n",
    "For tasks such as text classification, we may want to remove any unnecessary words and keep only words with meaning. \n",
    "\n",
    "Stopwords removal is not used in tasks such as machine translation or text summarization.\n",
    "\n",
    "Using [nltk](https://www.nltk.org/index.html) and [spaCy](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGkYhh5oMcx1"
   },
   "source": [
    "Stopwords removal using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idPo-mzrC2HW",
    "outputId": "fa02790a-d415-4375-a429-811526fc246a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "179\n",
      "{'won', 'my', 'being', 'under', \"wasn't\", \"needn't\", 'you', 'should', 'll', 'or', 'not', \"didn't\", \"should've\", \"that'll\", 'further', 'is', 'him', 'off', 'myself', 'of', 'mightn', 'didn', 'itself', 'so', 'which', 'most', 'an', 'i', 've', 'wouldn', 'do', 'both', 'but', 'into', 's', 'theirs', 'ourselves', 'above', 'd', 'have', \"you'd\", 'during', \"mightn't\", 'any', 'haven', 'they', 'ain', 'all', 'than', \"mustn't\", 'be', 'himself', 't', 'yours', 'for', 'hadn', 'what', \"hadn't\", 'does', 'here', 'just', 'couldn', \"don't\", \"isn't\", 'mustn', 'if', \"it's\", 'to', 'because', \"shan't\", 'ma', 'your', 'only', 'a', 'same', 'yourselves', 'too', 'y', 'me', 'has', 'out', 'there', 'once', 'needn', 'no', 'nor', 'why', 'from', 'can', 'until', 'herself', 'did', 'this', 'very', \"wouldn't\", 'its', 'when', 'don', 'own', 'hasn', 'few', 'and', 'having', 'hers', 'their', 'themselves', 'at', 'where', 'o', 'doesn', 'we', 'our', 'some', 'more', 'are', 'while', \"you'll\", 'against', 'each', 'who', \"weren't\", 'shouldn', \"won't\", 're', 'am', 'shan', \"she's\", 'ours', 'such', 'before', 'now', \"haven't\", 'she', \"aren't\", 'were', 'with', 'below', \"you've\", \"you're\", 'he', 'other', 'the', 'down', 'in', 'how', 'on', 'weren', 'm', 'through', 'again', 'as', 'wasn', 'these', 'then', \"couldn't\", \"hasn't\", 'aren', 'isn', 'yourself', 'about', 'his', 'after', 'over', 'was', 'between', 'that', 'up', \"shouldn't\", 'will', 'whom', 'it', 'been', 'those', 'doing', 'her', \"doesn't\", 'them', 'by', 'had'}\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "print(len(stop_words_nltk))\n",
    "print(stop_words_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lY7q0410L7t0",
    "outputId": "f0999a09-a3c4-4f0d-cf5c-da92bb8f4ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'jaws', '``', 'rare', 'film', 'grabs', 'attention', 'shows', 'single', 'image', 'screen', '.', 'movie', 'opens', 'blackness', ',', 'distant', ',', 'alien-like', 'underwater', 'sounds', '.', 'deserves', 'five', 'stars', ',', 'four', 'stars', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text_without_stopwords = [i for i in tokenized_text_nltk if not i in stop_words_nltk]\n",
    "print(tokenized_text_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux3FIzVTMYc7"
   },
   "source": [
    "Stopwords removal using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ys1CTPNoMgNv",
    "outputId": "9bc01d9f-b5d3-4127-d999-0384427c52b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "{'enough', 'becoming', \"'d\", 'must', 'sometimes', 'since', '‚Äôve', 'thereby', 'though', 'off', 'of', 'unless', 'due', 'next', 'various', 'amount', 'i', 'somewhere', 'do', 'but', 'thus', 'above', 'have', 'upon', 'during', 'all', 'nobody', 'yours', 'elsewhere', 'what', 'rather', 'together', 'another', 'whose', 'because', 'wherever', 'only', 'whereupon', 'too', 'across', 'often', 'no', 'really', 'besides', 'its', 'twelve', '‚Äôll', 'at', 'our', 'either', 'whether', 'are', 'whither', 're', 'used', 'along', 'else', 'he', 'in', 'forty', 'as', \"'ll\", 'beyond', 'take', 'us', 'whole', 'put', 'becomes', 'name', 'whereafter', 'about', 'amongst', 'seem', 'many', 'call', 'sometime', 'throughout', 'alone', 'whatever', 'was', 'up', 'cannot', 'whom', 'nine', 'down', 'one', 'fifteen', 'front', 'toward', 'moreover', 'whence', 'under', 'not', 'almost', 'thence', 'around', 'is', 'someone', '‚Äôd', 'whenever', 'five', '‚Äòve', 'most', 'an', 'please', \"'m\", 'ourselves', 'by', 'than', 'be', 'himself', 'therefore', 'formerly', 'yet', 'does', 'two', 'to', 'well', 'ten', 'always', 'keep', 'serious', 'mine', 'out', 'meanwhile', 'anything', 'several', 'wherein', 'within', '‚Äòm', 'can', 'from', 'why', 'until', 'did', 'when', 'hers', 'their', 'also', 'we', 'whoever', '‚Äôs', 'some', 'via', 'otherwise', 'regarding', 'done', 'now', 'everyone', 'she', 'seeming', 'would', 'the', 'afterwards', 'already', 'least', 'less', 'among', 'seems', 'using', 'except', 'say', 'after', 'seemed', 'back', 'will', 'side', 'everywhere', 'those', 'them', 'namely', 'never', 'move', 'you', 'should', 'or', 'third', 'anyone', 'him', 'behind', 'so', 'both', 'noone', 'empty', 'eight', \"'s\", 'part', 'any', 'they', 'somehow', 'for', 'bottom', 'here', 'just', 'even', 'a', 'yourselves', 'neither', 'me', 'nothing', 'became', 'herein', 'none', 'has', 'there', 'once', 'nor', 'top', '‚Äòd', 'hereupon', 'herself', 'whereas', 'hereby', 'own', 'thereupon', 'per', 'may', 'few', 'and', 'anywhere', 'made', 'themselves', 'where', 'give', 'four', 'each', 'how', 'again', 'former', 'onto', 'these', 'something', 'hereafter', 'over', 'twenty', 'although', 'it', 'might', 'been', 'every', 'doing', 'go', 'full', 'had', 'my', 'being', \"'ve\", '‚Äòre', 'three', 'therein', 'further', 'anyhow', 'myself', 'mostly', \"'re\", 'itself', 'which', 'indeed', 'others', 'latterly', 'into', \"n't\", 'anyway', 'whereby', 'n‚Äòt', 'first', 'if', 'sixty', 'could', 'become', 'beside', 'thereafter', 'six', 'your', 'eleven', 'same', 'nevertheless', 'hence', '‚Äôm', 'this', '‚Äòll', 'very', 'n‚Äôt', 'much', 'more', 'beforehand', 'while', 'against', 'who', 'hundred', 'nowhere', 'am', 'ours', 'such', 'perhaps', 'before', 'everything', 'were', 'with', 'thru', 'below', 'other', 'ever', 'latter', 'towards', 'without', 'on', 'through', 'last', 'still', 'then', 'quite', 'yourself', 'fifty', '‚Äôre', 'his', '‚Äòs', 'ca', 'see', 'show', 'between', 'that', 'make', 'get', 'her', 'however'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words_spacy = nlp.Defaults.stop_words\n",
    "print(len(stop_words_spacy))\n",
    "print(stop_words_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iE9rct__Oi7a",
    "outputId": "d4f2b80d-1db7-44fc-aef8-f4d946b755ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ , jaws, is, a, rare, film, that, grabs, your, attention, before, it, shows, you, a, single, image, on, screen, the, movie, opens, with, blackness, and, only, distant, alienlike, underwater, sounds, it, deserves, five, stars, not, four, stars]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text_spacy = nlp(text)\n",
    "tokenized_text_without_stopwords = [i for i in tokenized_text_spacy if not i in stop_words_spacy]\n",
    "print(tokenized_text_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH15qHZ3HYga"
   },
   "source": [
    "## Lematization/Stemming\n",
    "\n",
    "![1_HLQgkMt5-g5WO5VpNuTl_g.jpeg](https://miro.medium.com/max/564/1*HLQgkMt5-g5WO5VpNuTl_g.jpeg)\n",
    "\n",
    "[Photo source](https://tr.pinterest.com/pin/706854104005417976/)\n",
    "\n",
    "Using [nltk](https://www.nltk.org/index.html) and [spaCy](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOVeBOOYZo7F"
   },
   "source": [
    "Lematization\n",
    "\n",
    "Using the WordNetLemmatizer from nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E89_jM6NY_nh",
    "outputId": "7b7aa8f1-b0ba-4c88-f113-2a21e1bd95c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxu3jUM1UviR",
    "outputId": "5475234b-2849-4607-d816-f0fec285a315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaws jaw\n",
      "is is\n",
      "a a\n",
      "rare rare\n",
      "film film\n",
      "that that\n",
      "grabs grab\n",
      "your your\n",
      "attention attention\n",
      "before before\n",
      "it it\n",
      "shows show\n",
      "you you\n",
      "a a\n",
      "single single\n",
      "image image\n",
      "on on\n",
      "screen screen\n",
      "the the\n",
      "movie movie\n",
      "opens open\n",
      "with with\n",
      "blackness blackness\n",
      "and and\n",
      "only only\n",
      "distant distant\n",
      "alienlike alienlike\n",
      "underwater underwater\n",
      "sounds sound\n",
      "it it\n",
      "deserves deserves\n",
      "five five\n",
      "stars star\n",
      "not not\n",
      "four four\n",
      "stars star\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = word_tokenize(text)\n",
    "for word in words:\n",
    "    print(word, lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWPz08R0qD79"
   },
   "source": [
    "Using the [lemmatizer](https://spacy.io/api/lemmatizer) from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y5oA4G1muu4y",
    "outputId": "251c2a17-ee82-4c4e-f1d1-1dafe2be882d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
      "Collecting pip\n",
      "  Downloading pip-21.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.7 MB 5.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-58.2.0-py3-none-any.whl (946 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 946 kB 40.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.0)\n",
      "Installing collected packages: setuptools, pip\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 57.4.0\n",
      "    Uninstalling setuptools-57.4.0:\n",
      "      Successfully uninstalled setuptools-57.4.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.3\n",
      "    Uninstalling pip-21.1.3:\n",
      "      Successfully uninstalled pip-21.1.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Successfully installed pip-21.3 setuptools-58.2.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "pkg_resources"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.9 MB 5.1 MB/s            \n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42 kB 1.1 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.1 MB 39.5 MB/s            \n",
      "\u001b[?25hCollecting thinc<8.1.0,>=8.0.9\n",
      "  Downloading thinc-8.0.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (623 kB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 623 kB 51.8 MB/s            \n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (58.2.0)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456 kB 54.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 2.2.4\n",
      "    Uninstalling spacy-2.2.4:\n",
      "      Successfully uninstalled spacy-2.2.4\n",
      "Successfully installed catalogue-2.0.6 pathy-0.6.0 pydantic-1.8.2 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "catalogue",
         "spacy",
         "srsly",
         "thinc"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.6 MB 78 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (58.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.62.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.5\n",
      "    Uninstalling en-core-web-sm-2.2.5:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.5\n",
      "Successfully installed en-core-web-sm-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "11YnUMJpqEO3",
    "outputId": "be3a65b0-d181-45bb-eda7-689d30ee32d3"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0063e3451c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load English tokenizer, tagger, parser, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m def load(\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegacy_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mcatalogue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mavailable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "  print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMUayDqZweHR"
   },
   "source": [
    "Stemming in using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kN7dDNx1weU3"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for word in words:\n",
    "    print(word, ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmgsTVhIZuS9"
   },
   "source": [
    "[Other stemmers in nltk](https://www.nltk.org/api/nltk.stem.html)\n",
    "\n",
    "The spacy library does not perform stemming, only lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgozL531Z2Ju"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "To be uploaded here: https://forms.gle/ygCNwFM4i5RMPtsC6\n",
    "\n",
    "Preprocess texts from Twitter\n",
    "\n",
    "## Data\n",
    "\n",
    "We will use the twitter corpus from nltk, usually used in sentiment analysis.\n",
    "\n",
    "The fist step is downloading the dataset using the *download* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fLAUv1MRdHq",
    "outputId": "8b1f3b31-d2b8-40c1-b0e7-ed7048a383d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68_VdfEmjKXh"
   },
   "source": [
    "In order to inspect our data, we look at the first 25 tweets from the dataset. The text contains a lot of mentions, hashtags and emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIODYYSvSuWj",
    "outputId": "9acd1f46-3eac-4125-d997-0247990781ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
       " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
       " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
       " '@97sides CONGRATS :)',\n",
       " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days',\n",
       " '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM',\n",
       " \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\",\n",
       " '@Impatientraider On second thought, there‚Äôs just not enough time for a DD :) But new shorts entering system. Sheep must be buying.',\n",
       " 'Jgh , but we have to go to Bayan :D bye',\n",
       " 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell‚Ä¶ as the name implies :p.',\n",
       " '#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)',\n",
       " \"Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\",\n",
       " '@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)',\n",
       " \"@jjulieredburn Perfect, so you already know what's waiting for you :)\",\n",
       " 'Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0',\n",
       " 'Laying out a greetings card range for print today - love my job :-)',\n",
       " \"Friend's lunch... yummmm :)\\n#Nostalgia #TBS #KU.\",\n",
       " \"@RookieSenpai @arcadester it is the id conflict thanks for the help :D here's the screenshot of it working\",\n",
       " '@oohdawg_ Hi liv :))',\n",
       " 'Hello I need to know something can u fm me on Twitter?? ‚Äî sure thing :) dm me x http://t.co/W6Dy130BV7',\n",
       " '#FollowFriday @MBandScott_ @Eric_FLE @pointsolutions3 for being top new followers in my community this week :)',\n",
       " \"@rossbreadmore I've heard the Four Seasons is pretty dope. Penthouse, obvs #Gobigorgohome\\nHave fun y'all :)\",\n",
       " '@gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))',\n",
       " 'Hello :) Get Youth Job Opportunities follow &gt;&gt; @tolajobjobs @maphisa301',\n",
       " \"üíÖüèΩüíã - :)))) haven't seen you in years\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus  import twitter_samples\n",
    "\n",
    "tweets = twitter_samples.strings('positive_tweets.json')\n",
    "tweets = tweets[:25]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V_Y5pHtc07i"
   },
   "source": [
    "**Given a list of tweets, preprocess each tweet from the list.**\n",
    "\n",
    "**Instructions**: Implement the *preprocess* function. You can do the text cleaning in any order you prefer.\n",
    "\n",
    "**Hint**: You may need to use regex expressions (use the resources provided above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11qAmrMgS7QA",
    "outputId": "785ba7fa-193c-442d-d233-49730d381567"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['top engaged member community week',\n",
       " 'hey james odd please call contact centre able assist many thanks',\n",
       " 'listen last night bleed amazing track scotland',\n",
       " 'congrats',\n",
       " 'yeaaaah yippppy accnt verified rqst succeed got blue tick mark fb profile day',\n",
       " 'one irresistible',\n",
       " 'dont like keep lovely customer waiting long hope enjoy happy friday lwwf',\n",
       " 'second thought there enough time dd new short entering system sheep must buying',\n",
       " 'jgh go bayan bye',\n",
       " 'act mischievousness calling etl layer inhouse warehousing app katamari well name implies',\n",
       " 'top influencers community week',\n",
       " 'wouldnt love bigjuicyselfies',\n",
       " 'follow follow u back',\n",
       " 'perfect already know whats waiting',\n",
       " 'great new opportunity junior triathletes aged gatorade series get entry',\n",
       " 'laying greeting card range print today love job',\n",
       " 'friend lunch yummmm',\n",
       " 'id conflict thanks help here screenshot working',\n",
       " 'hi liv',\n",
       " 'hello need know something u fm twitter sure thing dm x',\n",
       " 'top new follower community week',\n",
       " 'ive heard four season pretty dope penthouse obvs fun yall',\n",
       " 'yeah suppose lol chat bit x',\n",
       " 'hello get youth job opportunity follow gtgt',\n",
       " 'havent seen year']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import emoji\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "      |\n",
    "      </?3                       # heart\n",
    "    )\"\"\"\n",
    "emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess(tweets):\n",
    "\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        tweets: a list of tweets\n",
    "    Output: \n",
    "        prepocessed_tweets: a list of preprocessed tweets\n",
    "    \"\"\"\n",
    "\n",
    "    ###you may need to create an additional list in which to store the processed tweets\n",
    "    ###pay attention that some of the cleaning steps can be done at the document level, while others may be computed at word level\n",
    "    prepocessed_tweets = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        ###remove new line characters '\\n'\n",
    "        ###remove links http://t.co/of3DyOzML0\n",
    "        ###remove mentions '@'\n",
    "        ###remove hashtags '#'\n",
    "        ###lowercase text\n",
    "        tweet = ' '.join([word.lower() for word in tweet.split() if word[:4] != \"http\" and word[0] not in ['@', '#']])\n",
    "\n",
    "        ###remove emojis and emoticons 'üëå üç≠ :) :D'\n",
    "        tweet = re.sub(emoticon_re, '', emoji.get_emoji_regexp().sub(u'', tweet))\n",
    "        \n",
    "        ###remove digits\n",
    "        tweet = re.sub(' \\d+', '', tweet)\n",
    "\n",
    "        ###remove punctuation\n",
    "        tweet = re.sub(r'[^\\w\\s]','', tweet)\n",
    "\n",
    "        ###tokenize tweet into separate words\n",
    "        ###remove stopwords\n",
    "        ###lematization or stemming\n",
    "        tweet = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(tweet) if word not in stop_words_nltk])\n",
    "\n",
    "        prepocessed_tweets += [tweet]\n",
    "\n",
    "    return prepocessed_tweets\n",
    "\n",
    "preprocess(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNPLCHObe0O5"
   },
   "source": [
    "Tools:\n",
    "\n",
    "* [Preprocessing library for Twitter](https://github.com/s/preprocessor)\n",
    "* [Emoji library](https://github.com/carpedm20/emoji)\n",
    "* [Demoji library](https://github.com/bsolomon1124/demoji)\n",
    "* [Gensim](https://radimrehurek.com/gensim/)\n",
    "\n",
    "\n",
    "Further reading:\n",
    "\n",
    "* [Lexical Normalization](https://arxiv.org/pdf/1710.03476.pdf)\n",
    "* [On learning and representing social meaning in NLP: a sociolinguistic perspective](https://aclanthology.org/2021.naacl-main.50.pdf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
