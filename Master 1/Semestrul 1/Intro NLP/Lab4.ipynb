{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXkuh5_mj3Wx"
   },
   "source": [
    "##Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pJqjzaGlQnT"
   },
   "source": [
    "Word representation methods from the last lab\n",
    "\n",
    "- Bag of Words\n",
    "- TF-IDF\n",
    "\n",
    "Limitations of these representations\n",
    "\n",
    "- High-dimensional\n",
    "- Sparse\n",
    "- No info about words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GPodGvnlyZX"
   },
   "source": [
    "Word2vec Paper [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkQ87w_smCcg"
   },
   "source": [
    "Word2Vec is a shallow, two-layer neural network which is trained to reconstruct linguistic contexts of words.\n",
    "\n",
    "It takes as its input a large corpus of words and produces a vector space, with each unique word in the corpus being assigned a corresponding vector in the space.\n",
    "\n",
    "\n",
    "Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
    "\n",
    "Example:    \n",
    "The **kid** studies mathematics.\n",
    "\n",
    "The **child** studies mathematics.\n",
    "\n",
    "![embedding](https://miro.medium.com/max/1400/1*sAJdxEsDjsPMioHyzlN3_A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7oue8cDnY1_"
   },
   "source": [
    "###Methods for building the Word2vec model\n",
    "\n",
    "![cbow-skip-gram](https://miro.medium.com/max/1400/1*cuOmGT7NevP9oJFJfVpRKA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KUil7cBnY4u"
   },
   "source": [
    "###Continuous Bag-of-Words (CBOW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hb12NPvv3Zww"
   },
   "source": [
    "CBOW predicts target words from the surrounding context words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfz-eLcNnY7f"
   },
   "source": [
    "![cbow](https://1.bp.blogspot.com/-nZFc7P6o3Yc/XQo2cYPM_ZI/AAAAAAAABxM/XBqYSa06oyQ_sxQzPcgnUxb5msRwDrJrQCLcBGAs/s1600/image001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZpZNbVynY91"
   },
   "source": [
    "###Skip-gram\n",
    "\n",
    "Skip-gram predicts surrounding context words from the target words.\n",
    "\n",
    "![skip-gram](https://i.stack.imgur.com/fYhXF.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38GGcsuG3s9_"
   },
   "source": [
    "##Architecture\n",
    "\n",
    "The words are feeded as one-hot vectors ( vector of the same length as the vocabulary, filled with zeros except at the index that represents the word we want to represent, which is assigned ‚Äú1‚Äù.)\n",
    "\n",
    "The hidden layer is a standard fully-connected (Dense) layer whose weights are the word embeddings.\n",
    "\n",
    "The output layer outputs probabilities for the target words from the vocabulary.\n",
    "\n",
    "The goal of this neural network is to learn the weights for the hidden layer matrix.\n",
    "\n",
    "![model](https://miro.medium.com/max/1400/1*tmyks7pjdwxODh5-gL3FHQ.png)\n",
    "\n",
    "High-level illustration of the architecture\n",
    "\n",
    "![model2](https://i.imgur.com/CBuZay5.png)\n",
    "\n",
    "The rows of the hidden layer weight matrix, are actually the word vectors (word embeddings).\n",
    "\n",
    "\n",
    "![hidden-layer](https://i.imgur.com/v6VqHad.png)\n",
    "\n",
    "The hidden layer operates as a lookup table. The output of the hidden layer is just the ‚Äúword vector‚Äù for the input word.\n",
    "\n",
    "More concretely, if you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just select the matrix row corresponding to the ‚Äò1‚Äô.\n",
    "\n",
    "![vector](https://i.imgur.com/EYhcA5S.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqCMFRaI6S2a"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZgTsN3Pmye0"
   },
   "source": [
    "###Semantic and syntactic relationships\n",
    "\n",
    "If different words are similar in context, then Word2Vec should have similar outputs when these words are passed as inputs, and in-order to have a similar outputs, the computed word vectors (in the hidden layer) for these words have to be similar, thus Word2Vec is motivated to learn similar word vectors for words in similar context.\n",
    "\n",
    "Word2Vec is able to capture multiple different degrees of similarity between words, such that semantic and syntactic patterns can be reproduced using vector arithmetic.\n",
    "\n",
    "![w2vec](https://i.imgur.com/I66L7No.png)\n",
    "\n",
    "![w2vec2](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/linear-relationships.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Now1mZyS_dYe"
   },
   "source": [
    "**Skip-gram** - works well with a small amount of the training data, represents well even rare words or phrases\n",
    "\n",
    "**CBOW** - several times faster to train than the skip-gram, slightly better accuracy for the frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSzy26Qw_dc5"
   },
   "source": [
    "###Word2vec embeddings in Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yuFIQwiLQkB5"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BUnYjhKAWoI"
   },
   "source": [
    "Gensim has multiple vector representations for words: word2vec, fasttext, glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7rV7zCyAHdJ",
    "outputId": "b7d9d77d-c425-40a2-9e9b-d7aab4bae297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo0dnaGaAesc"
   },
   "source": [
    "Downloading the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9XBIE2c_Aifx"
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acjBn5fQRoxb",
    "outputId": "b20c019d-3001-427b-8c44-76e77d6ca8bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
       "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656,\n",
       "        0.08056641, -0.5859375 , -0.00445557, -0.296875  , -0.01312256,\n",
       "       -0.08349609,  0.05053711,  0.15136719, -0.44921875, -0.0135498 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec['cat'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6-ySA4OBK3P",
    "outputId": "e1d54245-4682-42e7-85d3-4c114e8d68ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25689757"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similarity('dog', 'house')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDMjYXayBNB1",
    "outputId": "61cb7dc1-08bf-4dc7-f22d-8aa59dffba93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81064284"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similarity('dog', 'puppy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYtBf4mfBPsj",
    "outputId": "970864df-b91b-4825-be9a-5b9de2757280"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.8099379539489746),\n",
       " ('dog', 0.7609456777572632),\n",
       " ('kitten', 0.7464985251426697),\n",
       " ('feline', 0.7326233983039856),\n",
       " ('beagle', 0.7150583267211914),\n",
       " ('puppy', 0.7075453996658325),\n",
       " ('pup', 0.6934291124343872),\n",
       " ('pet', 0.6891531348228455),\n",
       " ('felines', 0.6755931377410889),\n",
       " ('chihuahua', 0.6709762215614319)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xixp7uGyEMTH"
   },
   "source": [
    "\n",
    "(king - man) + woman = queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sms3zhUKBRVH",
    "outputId": "d376d2be-86ea-4a5e-faa2-5237ac688a04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jogN9NS1VSHg",
    "outputId": "a4f8c6a5-bd6a-44dc-8656-bf7531e268d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Officer', 0.5694271326065063),\n",
       " ('officers', 0.538264274597168),\n",
       " ('offi_cer', 0.5283650159835815),\n",
       " ('chief', 0.48523107171058655),\n",
       " ('deputy', 0.47100305557250977),\n",
       " ('patrolwoman', 0.4685642719268799),\n",
       " ('policewoman', 0.46202757954597473),\n",
       " ('vice_president', 0.461116224527359),\n",
       " ('supervisor', 0.4552857577800751),\n",
       " ('oficer', 0.4532422721385956)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(['woman', 'officer'], negative = ['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIs9pHp3BZTT"
   },
   "source": [
    "##Assignment\n",
    "\n",
    "To be uploaded here: https://forms.gle/KuR71xiA2rR6tukz8 until December 15th.\n",
    "\n",
    "1. Play around with the word2vec model and see if there are any interesting or counterintuitive similarity results using  ```word2vec.similarity``` and ```word2vec.most_similar```.\n",
    "\n",
    "2. Use other embeddings (glove, fasttext) to encode the data from the sentiment analysis task and train the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "954luqj1ai_r"
   },
   "source": [
    "Using the word2vec embeddings from Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAXlt86TUCWc",
    "outputId": "048a0a97-6990-44dc-896d-ef86b534b419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kEMTQSnUwis",
    "outputId": "91959983-9ac8-4c1c-f2de-cf711c7c39ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus  import twitter_samples\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "print(len(pos_tweets))\n",
    "\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print(len(neg_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jLZO6yXUy7T"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pos_df = pd.DataFrame(pos_tweets, columns = ['tweet'])\n",
    "pos_df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NDhKlXNU0Hk"
   },
   "outputs": [],
   "source": [
    "neg_df = pd.DataFrame(neg_tweets, columns = ['tweet'])\n",
    "neg_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgQua4ukU1Jl"
   },
   "outputs": [],
   "source": [
    "data_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "# data_df = data_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGKhcf-QU3Gy",
    "outputId": "e5fb2d15-5397-4e7f-ce58-5ec3c0f7d103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweet  label\n",
      "6745                 @sweetbabecake yea i guess so :(((      0\n",
      "8859  I fucking hate when I wake up like at this tim...      0\n",
      "644         .@sajidislam honored to have you here ! :-)      1\n",
      "6013                    o otp :( http://t.co/EVislmNp5V      0\n",
      "2283  @effinoreos HAPPY 15th BIRTHDAY VIANEY!!! (a.k...      1\n",
      "...                                                 ...    ...\n",
      "4840  @scousebabe888 Nice Holiday Honey!!!!!!!!!!!!!...      1\n",
      "3976  @planetjedward GoodMorning ! What's coming nex...      1\n",
      "7151  I feel like I'm a weird person for shipping Be...      0\n",
      "6902  I met a new kinds of people, new classmate, ne...      0\n",
      "5326        @hamzaabasiali exactly but unfortunately :(      0\n",
      "\n",
      "[8000 rows x 2 columns]\n",
      "                                                  tweet  label\n",
      "5648  @jenxmish @wittykrushnic you are the only thin...      0\n",
      "7425                                   Omg no Amber :((      0\n",
      "255   @AvinPera  follow @jnlazts &amp; http://t.co/R...      1\n",
      "305   Hi BAM ! @BarsAndMelody \\nCan you follow my be...      1\n",
      "1427              @TheMattEspinosa you make me happy :)      1\n",
      "...                                                 ...    ...\n",
      "6279  @ellierowexo in this weather, are you mad? &am...      0\n",
      "7960                                  I WANT üçúüçúüçúüçúüçú :(((      0\n",
      "8655  idk where I went wrong I used to be so cute :(...      0\n",
      "5276                   @ABeezyGMT says the Man U fan :(      0\n",
      "5329                                     Aww too bad :(      0\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2, shuffle = True)\n",
    "print(train_df)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UdnpkcGXmZv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def compute_embeddings(df):\n",
    "    train_emb = []\n",
    "    for i, row in tqdm.tqdm(df.iterrows(), total = len(df.index)):\n",
    "        words = row['tweet'].split(' ')\n",
    "        words = filter(lambda x: x in word2vec.vocab, words)\n",
    "        text_emb = [word2vec[word] for word in words]\n",
    "        \n",
    "        if len(text_emb) == 0:\n",
    "            train_emb.append(np.zeros(300))\n",
    "            continue\n",
    "\n",
    "        doc_embedding = np.mean(text_emb, axis = 0)\n",
    "        train_emb.append(doc_embedding)\n",
    "    return np.array(train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o65sQbtMS66a",
    "outputId": "263af0e8-f602-460a-b616-360d05da8de8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [00:01<00:00, 4282.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 4114.12it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_emb = compute_embeddings(train_df)\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_test_emb = compute_embeddings(test_df)\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcflA2mDU79N",
    "outputId": "fa9e364b-6bf1-457d-9645-4cf085e3d69e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(verbose=2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(verbose = 2)\n",
    "svm.fit(X_train_emb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7o_5xGfYJxG",
    "outputId": "dfc5f2ff-d3af-489b-ac37-f36ebf57f13b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8815\n",
      "Precision 0.9507803121248499\n",
      "F1 score 0.869851729818781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score \n",
    "\n",
    "y_test_pred = svm.predict(X_test_emb)\n",
    "\n",
    "print('Accuracy', accuracy_score(y_test, y_test_pred))\n",
    "print('Precision',precision_score(y_test, y_test_pred))\n",
    "print('F1 score',f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftFqAxWFlT_J"
   },
   "source": [
    "Notebook adapted from https://israelg99.github.io/2017-03-23-Word2Vec-Explained/\n",
    "\n",
    "Further Reading\n",
    "\n",
    "- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/pdf/1607.06520.pdf)\n",
    "- [Debiaswe: try to make word embeddings less sexist](https://github.com/tolga-b/debiaswe)\n",
    "- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "- [Fasttext Word vectors for 157 languages](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "- [Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejKXRanpJHWB"
   },
   "source": [
    "## 1. Word2vec similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Y53yXG_JCZY",
    "outputId": "5f4b0060-dbcd-4e37-800e-13f5833219bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.5393532514572144),\n",
       " ('guy', 0.47399765253067017),\n",
       " ('Alexios_Marakis', 0.4579210579395294),\n",
       " ('Man', 0.4575732350349426),\n",
       " ('teenager', 0.4346425235271454),\n",
       " ('dude', 0.42896467447280884),\n",
       " ('woman', 0.42854905128479004),\n",
       " ('suspected_purse_snatcher', 0.4247782826423645),\n",
       " ('him', 0.42347830533981323),\n",
       " ('motorcyclist', 0.4231083393096924)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(['king', 'man'], negative = ['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBQpqhVWdGe5",
    "outputId": "982e4d21-d7e1-4734-8d9f-eb435e2404fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Earth', 0.7105128765106201),\n",
       " ('planet', 0.6802847981452942),\n",
       " ('meek_inheriting', 0.5625146627426147),\n",
       " ('earths', 0.5312458276748657),\n",
       " ('cosmos', 0.5272278785705566),\n",
       " ('mankind', 0.5163297057151794),\n",
       " ('mega_vertebrate', 0.5102849006652832),\n",
       " ('shepherded_Tolkien_Middle', 0.5001775026321411),\n",
       " ('ERDAS_creates', 0.4907360076904297),\n",
       " ('Martian_surface', 0.480654239654541)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('earth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_KQjNbbbenb",
    "outputId": "43fc83d1-9260-4e71-f529-b6adc21e8767"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vampire_werewolf', 0.3629043996334076),\n",
       " ('love_triangle', 0.35874736309051514),\n",
       " ('vampire', 0.3576195240020752),\n",
       " ('werewolf_Jacob', 0.3553285598754883),\n",
       " ('Harry_Daniel_Radcliffe', 0.3486817479133606),\n",
       " ('plotline', 0.3475787937641144),\n",
       " ('Bella_Swan', 0.3428325355052948),\n",
       " ('Edward_Cullen', 0.3365398049354553),\n",
       " ('WEIRD_SCIENCE', 0.33004915714263916),\n",
       " ('vampires', 0.32982370257377625)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(['werewolf'], negative = ['wolf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwHbmiC0cBGY",
    "outputId": "a1d56d2a-7e36-4af6-e1b4-cec5c66a03bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.5253201723098755),\n",
       " ('teenager', 0.4768238067626953),\n",
       " ('teenage_girl', 0.4647809565067291),\n",
       " ('boy', 0.4555785655975342),\n",
       " ('PARANOID_schizophrenic', 0.44646579027175903),\n",
       " ('horribly_horribly_deranged', 0.4370340406894684),\n",
       " ('girl', 0.43483102321624756),\n",
       " ('transvestite_hooker', 0.42668646574020386),\n",
       " ('SUSPECT_SOUGHT', 0.4237041771411896),\n",
       " ('nightclubber', 0.4223080575466156)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(['werewolf', 'man'], negative = ['wolf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcJaXWr9FFYw"
   },
   "source": [
    "## 2. Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSo27z_pGHyx",
    "outputId": "69dde43c-f757-490c-fcf0-9702acd92c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdS5rFGXGIva",
    "outputId": "f4460f62-ebfe-4cd1-d73c-4a43b3adcddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus  import twitter_samples\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "print(len(pos_tweets))\n",
    "\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print(len(neg_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quEdT6-dGPUQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pos_df = pd.DataFrame(pos_tweets, columns = ['tweet'])\n",
    "pos_df['label'] = 1\n",
    "\n",
    "neg_df = pd.DataFrame(neg_tweets, columns = ['tweet'])\n",
    "neg_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cK5HPkBPGQme"
   },
   "outputs": [],
   "source": [
    "data_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "# data_df = data_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpIfr4tzGatB",
    "outputId": "4ba39b1b-eb0e-4e7f-bab4-445a3822e5a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweet  label\n",
      "9132                   7pm on a Friday and I am dead :(      0\n",
      "875   @TOBMAST3R @inspchin @koeitecmoeurope @TanikoH...      1\n",
      "1353  i thought they won at mubank.. lol \\nbecause d...      1\n",
      "7916                              @crude18 takfaham. :(      0\n",
      "6976  No Friday night live @NRL on @GEMChannel in St...      0\n",
      "...                                                 ...    ...\n",
      "9759                                 something wrong :(      0\n",
      "2477  @rusmexuswriters I have seen some of the new f...      1\n",
      "5839  this place will forever hold a very special pl...      0\n",
      "9383  @seokielips sorry :( i spend more time on inst...      0\n",
      "1319  @SilverArrowsHAM Are you seeing the cars drift...      1\n",
      "\n",
      "[8000 rows x 2 columns]\n",
      "                                                  tweet  label\n",
      "6433                                         Iam fat :(      0\n",
      "9319          @BenJPierce I wish I could see you Ben :(      0\n",
      "3450  @paulbeaton720 Looking good! Let us know what ...      1\n",
      "5326        @hamzaabasiali exactly but unfortunately :(      0\n",
      "444   HOME - AmassiveoverdoseofshipS - http://t.co/2...      1\n",
      "...                                                 ...    ...\n",
      "2207         @debcridland hey you survived the train :D      1\n",
      "6723    Really wish I was out to d.c w my friends rn :(      0\n",
      "265   ...and the Rakyat will keep visiting your blog...      1\n",
      "9268  @ZhenJie3007 yea man... Suppose to football td...      0\n",
      "7724  Konga\"@BeemanNONI: Well... \"@_topeh: ni wa*\"@i...      0\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2, shuffle = True)\n",
    "print(train_df)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNaRolRPFbF-",
    "outputId": "8aa6e76a-6442-47e8-ad92-dacacb2a95ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 399999/400000 [00:46<00:00, 8611.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "glove = vocab.GloVe(name='6B', dim=300)\n",
    "\n",
    "print('Loaded {} words'.format(len(glove.itos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lerfbX3fFXs4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def compute_embeddings(df):\n",
    "    train_emb = []\n",
    "    for i, row in tqdm.tqdm(df.iterrows(), total = len(df.index)):\n",
    "        words = row['tweet'].split(' ')\n",
    "        words = filter(lambda x: x in glove.stoi.keys(), words)\n",
    "        text_emb = [np.asarray(glove.vectors[glove.stoi[word]], dtype='float32') for word in words]\n",
    "        \n",
    "        if len(text_emb) == 0:\n",
    "            train_emb.append(np.zeros(300))\n",
    "            continue\n",
    "\n",
    "        doc_embedding = np.mean(text_emb, axis = 0)\n",
    "        train_emb.append(doc_embedding)\n",
    "    return np.array(train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnOf8-sHGCsf",
    "outputId": "0d566c67-0563-488b-e921-0e0280bcdc47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [00:01<00:00, 4192.58it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 4268.96it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_emb = compute_embeddings(train_df)\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_test_emb = compute_embeddings(test_df)\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HEBVOcuWGkxI",
    "outputId": "ec4384e0-74e6-4285-b5fe-a83312032252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(verbose=2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(verbose = 2)\n",
    "svm.fit(X_train_emb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "937OQvE3GpSl",
    "outputId": "b01e75fd-8953-4620-e4c6-1ace24f74479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9015\n",
      "Precision 0.8747609942638623\n",
      "F1 score 0.9028120374938332\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score \n",
    "\n",
    "y_test_pred = svm.predict(X_test_emb)\n",
    "\n",
    "print('Accuracy', accuracy_score(y_test, y_test_pred))\n",
    "print('Precision',precision_score(y_test, y_test_pred))\n",
    "print('F1 score',f1_score(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
