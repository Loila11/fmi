{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "words = ['laptop', 'laptops', 'school', 'student', 'NLP', 'going', 'studying', 'reading', 'book', 'books', 'programming',\n",
    "        'linguistics', 'maths', 'enjoy', 'experiment', 'learning', 'psychology', 'statistics', 'model', 'academic', 'AI',\n",
    "        'research', 'computer', 'mechanics', 'robot', 'NASA', 'philosophy', 'agent', 'strategy', 'computation', 'logic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each synset of each word find the corresponding Wikipedia Page mapping and print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def lists_to_list(lists):\n",
    "    return list(itertools.chain(*lists))\n",
    "\n",
    "\n",
    "def parse_sentence(sentence):\n",
    "    return [word.lower() for word in word_tokenize(sentence) \n",
    "            if word.lower() not in stop_words and word[0] not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonyms(synset):\n",
    "    return synset.lemma_names()\n",
    "\n",
    "\n",
    "def hypernyms(synset):\n",
    "    return lists_to_list([synonyms(hypernym) for hypernym in synset.hypernyms()])\n",
    "\n",
    "\n",
    "def hyponyms(synset):\n",
    "    return lists_to_list([synonyms(hyponym) for hyponym in synset.hyponyms()])\n",
    "\n",
    "\n",
    "def sisters(synset):\n",
    "    hypernyms = synset.hypernyms()\n",
    "    return [] if len(hypernyms) == 0 else \\\n",
    "        lists_to_list([synonyms(sister) for sister in hypernyms[0].hyponyms() if sister != synset])\n",
    "\n",
    "\n",
    "def gloss(synset):\n",
    "    return parse_sentence(synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get wikipedia info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r_session = requests.Session()\n",
    "\n",
    "URL = 'https://en.wikipedia.org/w/api.php'\n",
    "PARAMS = {\n",
    "    'action': 'query',\n",
    "    'titles': '',\n",
    "    'prop': '',\n",
    "    'format': 'json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_ctx(word, feature):\n",
    "    PARAMS['titles'] = word\n",
    "    PARAMS['prop'] = feature    # 'redirects', 'links', 'categories'\n",
    "\n",
    "    query_response = r_session.get(url=URL, params=PARAMS)\n",
    "    json_data = query_response.json()\n",
    "\n",
    "    wikipedia_pages = list(json_data['query']['pages'].values())[0]\n",
    "\n",
    "    if feature in wikipedia_pages.keys():\n",
    "        return wikipedia_pages[feature]\n",
    "    return [wikipedia_pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def parse_titles(features):\n",
    "    return list(set(lists_to_list([parse_sentence(sentence['title']) for sentence in features])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ctx_s(synset):\n",
    "    return set(synonyms(synset) + hypernyms(synset) + hyponyms(synset) + sisters(synset) + gloss(synset))\n",
    "\n",
    "\n",
    "def ctx_w(word):\n",
    "    return set(parse_titles(wiki_ctx(word, 'links')) + parse_titles(wiki_ctx(word, 'categories')))\n",
    "\n",
    "\n",
    "def score(synset, word):\n",
    "    ctxs = ctx_s(synset)\n",
    "    ctxw = ctx_w(word)\n",
    "\n",
    "    return len([0 for word in ctxs if word in ctxw]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_word(word, synset, page):\n",
    "    f = open('output.txt', 'a')\n",
    "    f.write(f'{word}: {synset.definition()} - https://en.wikipedia.org/wiki/{page}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1(ctx, words):\n",
    "    for word in words:\n",
    "        synsets = wordnet.synsets(word)\n",
    "        wikipedia_pages = wiki_ctx(word, 'redirects')\n",
    "        if len(synsets) == 1 and len(wikipedia_pages) == 1:\n",
    "            page = '_'.join(wikipedia_pages[0]['title'].split())\n",
    "            ctx[word] = [(synsets[0], page)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2(ctx, word, wikipedia_pages):\n",
    "    for redirect in wikipedia_pages:\n",
    "        for synsets in ctx.values():\n",
    "            for synset, page in synsets:\n",
    "                try:\n",
    "                    if page == redirect['title'] and synset in wordnet.synsets(word):\n",
    "                        page = '_'.join(page.split())\n",
    "                        ctx[word] = [(synset, page)]\n",
    "                        print_word(word, synset, page)\n",
    "                        return True\n",
    "                except KeyError:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3(ctx, word, synsets, wikipedia_pages):\n",
    "    ctx[word] = []\n",
    "    for synset in synsets:\n",
    "        p_max = (0, '', '')\n",
    "        for page in wikipedia_pages:\n",
    "            p = score(synset, page['title'])\n",
    "            if p > p_max[0]:\n",
    "                p_max = (p, page['title'])\n",
    "\n",
    "        page = '_'.join(p_max[1].split())\n",
    "        ctx[word].append((synset, page))\n",
    "        print_word(word, synset, page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mapping(words):\n",
    "    ctx = {}\n",
    "    step1(ctx, words)\n",
    "\n",
    "    for word in words:\n",
    "        if word in ctx.keys():\n",
    "            print_word(word, ctx[word][0][0], ctx[word][0][1])\n",
    "            continue\n",
    "\n",
    "        synsets = wordnet.synsets(word)\n",
    "        wikipedia_pages = wiki_ctx(word, 'redirects')\n",
    "\n",
    "        if step2(ctx, word, wikipedia_pages):\n",
    "            continue\n",
    "\n",
    "        step3(ctx, word, synsets, wikipedia_pages)\n",
    "\n",
    "    return ctx\n",
    "\n",
    "\n",
    "ctx = mapping(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
