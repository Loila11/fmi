{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\mirun\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from wikipedia) (2.22.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from wikipedia) (4.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.8)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (1.9.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mirun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mirun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\mirun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mirun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "modelPath = './GoogleNews-vectors-negative300.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(modelPath, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nlp = wikipedia.page(\"Natural language processing\")\n",
    "original_text = nltk.tokenize.sent_tokenize(nlp.content)[:19]\n",
    "text = [word for word in nltk.word_tokenize(' '.join(original_text)) if word[0] not in string.punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Print the number of words in the model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 3000000\n"
     ]
    }
   ],
   "source": [
    "print(f'Model size: {model.vectors.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Print all the words in the text that do not appear in the model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'and', '1966', '1970s', 'ALPAC', 'well-summarized', 'ten-year-long', 'of', 'chatterbots', 'â€“', '1978', '1980s', '1981', 'human-like', 'real-world', 'QUALM', '1975', '1990s', '1960s', 'Rogerian', 'natural-language', '1954', '1950s', '1979', 'hey-day', '1964', '1950', 'e.g.', 'computer-understandable', '1976', '1977', 'SHRDLU', 'to'}\n"
     ]
    }
   ],
   "source": [
    "outliers = set()\n",
    "\n",
    "for word in text:\n",
    "    if word not in model:\n",
    "        outliers.add(word)\n",
    "\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Which are the two most distant words in the text, and which are the closest? Print the distance too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most distant words: \"further\", \"MARGIE\" at distance: -0.18977756798267365\n",
      "Closest words: \"three\", \"five\" at distance: 0.9370177388191223\n"
     ]
    }
   ],
   "source": [
    "text = list(set(text) - outliers)\n",
    "n = len(text)\n",
    "\n",
    "min_similarity = (1, '', '')\n",
    "max_similarity = (-1, '', '')\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        similarity = model.similarity(text[i], text[j])\n",
    "        if similarity > max_similarity[0]:\n",
    "            max_similarity = (similarity, text[i], text[j])\n",
    "\n",
    "        if similarity < min_similarity[0]:\n",
    "            min_similarity = (similarity, text[i], text[j])\n",
    "\n",
    "print(f'Most distant words: \"{min_similarity[1]}\", \"{min_similarity[2]}\" at distance: {min_similarity[0]}')\n",
    "print(f'Closest words: \"{max_similarity[1]}\", \"{max_similarity[2]}\" at distance: {max_similarity[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using NER (Named Entity Recognition) find the named entities in the text. Print the first 5 most similar words to them both in upper and lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alan_Turing', 'Machinery', 'Lehnert', 'Carbonell', 'Cullingford', 'ALPAC', 'Plot_Units', 'Russian', 'John_Searle', 'Symbolic', 'Schank', 'Meehan', 'Politics', 'Turing', 'QUALM', 'Rogerian', 'NLP', 'Georgetown', 'Chinese', 'TaleSpin', 'PAM', 'MARGIE', 'Little', 'SAM', 'Joseph_Weizenbaum', 'Intelligence', 'English', 'ELIZA', 'Wilensky', 'PARRY', 'SHRDLU']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "ner = []\n",
    "\n",
    "for sentence in original_text:\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner += ['_'.join(c[0] for c in chunk)]\n",
    "\n",
    "ner = list(set(ner))\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity for the word: Alan_Turing\n",
      "Most similar in uppercase: ['Turing', 'Charles_Babbage', 'mathematician_Alan_Turing', 'pioneer_Alan_Turing', 'On_Computable_Numbers']\n",
      "Most similar in lowercase: \n",
      "\n",
      "Similarity for the word: Machinery\n",
      "Most similar in uppercase: ['Machine_Tool', 'Machinery_SHB_######', 'Machine_Tools', 'Equipment', 'Hydraulics']\n",
      "Most similar in lowercase: ['machineries', 'equipment', 'Machinery', 'maker_Komatsu_Ltd.', 'Komatsu_####.T']\n",
      "\n",
      "Similarity for the word: Cullingford\n",
      "Most similar in uppercase: ['Frappell', 'Skilbeck', 'Griffiths', 'Mayled', 'Forteath']\n",
      "Most similar in lowercase: \n",
      "\n",
      "Similarity for the word: ALPAC\n",
      "Most similar in uppercase: \n",
      "Most similar in lowercase: \n",
      "\n",
      "Similarity for the word: Russian\n",
      "Most similar in uppercase: ['Ukrainian', 'Russia', 'Kazakh', 'Belarusian', 'Belarussian']\n",
      "Most similar in lowercase: ['russia', 'moscow', 'british', 'russians', 'poland']\n",
      "\n",
      "Similarity for the word: John_Searle\n",
      "Most similar in uppercase: \n",
      "Most similar in lowercase: \n",
      "\n",
      "Similarity for the word: Symbolic\n",
      "Most similar in uppercase: ['symbolic', 'Symbols', 'Transcends', 'Poignant', 'Symbolism']\n",
      "Most similar in lowercase: ['symbolically', 'symbolic_gesture', 'symbolism', 'symbolical', 'largely_symbolic']\n",
      "\n",
      "Similarity for the word: Schank\n",
      "Most similar in uppercase: ['Zarling', 'Vanskike', 'McNett', 'Stiman', 'Fernbaugh']\n",
      "Most similar in lowercase: \n",
      "\n",
      "Similarity for the word: Turing\n",
      "Most similar in uppercase: ['Alan_Turing', 'Turing_Test', 'mathematician_Alan_Turing', 'GÃ¶del', 'Charles_Babbage']\n",
      "Most similar in lowercase: ['Â¬', 'ing', 'nds', 'tion', 'cks']\n",
      "\n",
      "Similarity for the word: Rogerian\n",
      "Most similar in uppercase: \n",
      "Most similar in lowercase: \n",
      "\n",
      "Similarity for the word: NLP\n",
      "Most similar in uppercase: ['neuro_linguistic_programming', 'Neuro_Linguistic_Programming', 'Master_Practitioner', 'Neuro_Linguistic_Programming_NLP', 'NLP_Neuro_Linguistic_Programming']\n",
      "Most similar in lowercase: \n",
      "\n",
      "Similarity for the word: Georgetown\n",
      "Most similar in uppercase: ['Villanova', 'Princeton', 'Towson', 'George_Mason', 'Syracuse']\n",
      "Most similar in lowercase: [':)_RT_@', 'adrian', 'lancaster', 'princeton', 'syracuse']\n",
      "\n",
      "Similarity for the word: Chinese\n",
      "Most similar in uppercase: ['China', 'Taiwanese', 'Beijing', 'Zhang', 'Li']\n",
      "Most similar in lowercase: ['japanese', 'asian', 'american', 'indian', 'thai']\n",
      "\n",
      "Similarity for the word: PAM\n",
      "Most similar in uppercase: ['PLP', 'FNM', 'UPP', 'JLP', 'SKNLP']\n",
      "Most similar in lowercase: ['gemma', 'jh', \"o'brien\", 'susan', 'newsdesk@afxnews.com_nma']\n",
      "\n",
      "Similarity for the word: MARGIE\n",
      "Most similar in uppercase: ['MARJORIE', 'KRISTI', 'MAURA', 'CANDACE', 'ADRIENNE']\n",
      "Most similar in lowercase: \n",
      "\n",
      "Similarity for the word: Little\n",
      "Most similar in uppercase: ['LIttle', 'Alan_Arkin_Little', 'Abigail_Breslin_Little', \"Li'l\", 'â€¢_Gastineau_Channel']\n",
      "Most similar in lowercase: ['bit', 'much', 'wee_bit', 'litte', \"everything'sa\"]\n",
      "\n",
      "Similarity for the word: SAM\n",
      "Most similar in uppercase: ['CARL', 'DT_##mm_F#.#', 'MATT', 'CHRIS', 'KEN']\n",
      "Most similar in lowercase: ['samuel', 'kim', 'ryan', 'jeff', 'raymond']\n",
      "\n",
      "Similarity for the word: Joseph_Weizenbaum\n",
      "Most similar in uppercase: ['Erwin_Schrodinger', 'Arnold_Stang', 'Jef_Raskin', 'Raymond_Kurzweil', 'Norbert_Wiener']\n",
      "Most similar in lowercase: \n",
      "\n",
      "Similarity for the word: Intelligence\n",
      "Most similar in uppercase: ['intelligence', 'CMR_TNS_Media', 'Intelligence_Agency', 'Intelligence_directorate', 'Minister_Ali_Fallahian']\n",
      "Most similar in lowercase: ['Intelligence', 'intel', 'CIA', 'counterintelligence', 'Alain_Chouet']\n",
      "\n",
      "Similarity for the word: English\n",
      "Most similar in uppercase: ['english', 'Engish', 'Funeral_Home_Oakmont', 'Malaya_Gruzinskaya_M._Barrikadnaya', 'language']\n",
      "Most similar in lowercase: ['Institute_ITRI_eng', 'English', 'spanish', 'grammer', 'japanese']\n",
      "\n",
      "Similarity for the word: ELIZA\n",
      "Most similar in uppercase: ['NOELLE', 'MINDY', 'MISTY', 'INGRID', 'CASSIE']\n",
      "Most similar in lowercase: \n",
      "\n",
      "Similarity for the word: SHRDLU\n",
      "Most similar in uppercase: \n",
      "Most similar in lowercase: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in ner:\n",
    "    uppercase = [elem[0] for elem in model.most_similar(word)[:5]] if word in model else ''\n",
    "    lowercase = [elem[0] for elem in model.most_similar(word.lower())[:5]] if word.lower() in model else ''\n",
    "\n",
    "    print(f'Similarity for the word: {word}\\n' + \\\n",
    "          f'Most similar in uppercase: {uppercase}\\n' + \\\n",
    "          f'Most similar in lowercase: {lowercase}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Print the clusters of words that are the most similar in the text (you can use sklearn's Kmeans) based on their vectors in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n",
      "Cluster: ['Symbolic', 'symbolic']\n",
      "Label: 1\n",
      "Cluster: ['interactions', 'intelligence', 'applying', 'Turing', 'published', 'Intelligence', 'PARRY', 'science', 'accurately', 'Machinery', 'Challenges', 'human', 'example', 'research', 'generic', 'collection', 'authors', 'premise', 'generation', 'system', 'statistical', 'experiment', 'machine', 'Plot', 'computer', 'MARGIE', 'ELIZA', 'subfield', 'worlds', 'recognition', 'interaction', 'articulated', 'titled', 'speech', 'processing', 'psychotherapist', 'analyze', 'Natural', 'Weizenbaum', 'technology', 'developed', 'PAM', 'Units', 'program', 'History', 'automated', 'criterion', 'emulates', 'conceptual', 'article', 'extract', 'test', 'conducted', 'data', 'computers', 'simulation', 'Computing', 'systems', 'write', 'artificial', 'report', 'categorize', 'Politics', 'Chinese', 'written', 'structured', 'methods', 'SAM', 'programmers']\n",
      "Label: 2\n",
      "Cluster: ['When', 'Why', 'However', 'Some', 'My', 'Given', 'Examples', 'During', 'The', 'Using', 'Already']\n",
      "Label: 3\n",
      "Cluster: ['from', 'three', 'now', 'not', 'themselves', 'further', 'had', 'with', 'five', 'small', 'its', 'you', 'years', 'automatic', 'how', 'matching', 'though', 'emotion', 'was', 'it', 'Schank', 'were', 'by', 'Georgetown', 'well', 'much', 'sometimes', 'natural', 'base', 'goal', 'would', 'early', 'startlingly', 'be', 'very', 'TaleSpin', 'an', 'time', 'in', 'head', 'provided', 'including', 'which', 'your', 'Meehan', 'within', 'hurts', 'rules', 'what', 'no', 'Little', 'mark', 'many', 'real', 'Wilensky', 'slower', 'might', 'after', 'patient', 'proposed', 'fully', 'particular', 'began', 'or', 'frequently', 'has', 'Carbonell', 'dramatically', 'Searle', 'can', 'Russian', 'Alan', 'first', 'blocks', 'roots', 'say', 'this', 'called', 'sixty', 'into', 'for', 'then', 'about', 'failed', 'Lehnert', 'as', 'Joseph', 'thought', 'large', 'almost', 'at', 'claimed', 'room', 'exceeded', 'than', 'reduced', 'is', 'expectations', 'them', 'when', 'Cullingford', 'more', 'those', 'late', 'the', 'notably', 'that', 'found', 'between', 'do', 'John', 'other', 'until']\n",
      "Label: 4\n",
      "Cluster: ['interpretation', 'linguistics', 'understanding', 'contextual', 'translation', 'contents', 'insights', 'English', 'nuances', 'vocabularies', 'NLP', 'phrasebook', 'ontologies', 'knowledge', 'language']\n",
      "Label: 5\n",
      "Cluster: ['successful', 'documents', 'problem', 'tasks', 'provide', 'amounts', 'response', 'fulfill', 'involves', 'involved', 'progress', 'working', 'concerned', 'questions', 'task', 'process', 'solved', 'organize', 'separate', 'are', 'restricted', 'funding', 'sentences', 'capable', 'answers', 'contained', 'responding', 'involve', 'confronts', 'information', 'includes']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "model['When']\n",
    "X = np.array([model[word] for word in text])\n",
    "\n",
    "n_clusters = 6\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "for j in range(n_clusters):\n",
    "    cluster = []\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        if kmeans.labels_[i] == j:\n",
    "            cluster += [text[i]]\n",
    "\n",
    "    print(f'Label: {j}\\nCluster: {cluster}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
