{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synset(word):\n",
    "    return wordnet.synsets(word)[0]\n",
    "\n",
    "def get_synset_list(words):\n",
    "    synsets = []\n",
    "    for word in words:\n",
    "        synsets += [get_synset(word)]\n",
    "    return synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Original Lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_gloss(gloss: str):\n",
    "    gloss_list = word_tokenize(gloss)\n",
    "    gloss_processed = [word.lower() for word in gloss_list \n",
    "                       if word.lower() not in stop_words and word[0] not in string.punctuation]\n",
    "    return gloss_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloss_score(synset1: str, synset2: str):\n",
    "    gloss1 = list(set(process_gloss(synset1)))\n",
    "    gloss2 = list(set(process_gloss(synset2)))\n",
    "    score = 0\n",
    "\n",
    "    for word in gloss1:\n",
    "        if word in gloss2:\n",
    "            score += 1\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_score_glosses(synset1: str, synsets2: [str], gloss_score_func = gloss_score):\n",
    "    best_score = 0\n",
    "\n",
    "    for synset2 in synsets2:\n",
    "        score = gloss_score_func(synset1, synset2)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    sentence_pos = pos_tag(sentence)\n",
    "    return [wordnet.synsets(word[0].lower(), 'a' if word[1][0].lower() == 'j' else word[1][0].lower()) \n",
    "            for word in sentence_pos if word[0].lower() not in stop_words and word[0][0] not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_lesk(sentence: str, word: str, pos: str):\n",
    "    target_synsets = wordnet.synsets(word, pos)\n",
    "    sentence_processed = process_sentence(sentence)\n",
    "\n",
    "    best_score = (-1, '')\n",
    "    for synset in target_synsets:\n",
    "        score = 0\n",
    "\n",
    "        for word in sentence_processed:\n",
    "            glosses = [synset.definition() for synset in word]\n",
    "            score += best_score_glosses(synset.definition(), glosses)\n",
    "\n",
    "        if score > best_score[0]:\n",
    "            best_score = (score, synset)\n",
    "\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified lesk definition:\n",
      "an educational institution's faculty and students\n",
      "Original lesk definition:\n",
      "a body of creative artists or writers or thinkers linked by a similar style or by similar teachers\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "sentence = word_tokenize('Students enjoy going to school, studying and reading books')\n",
    "\n",
    "simplified_lesk_sense = lesk(sentence, 'school', 'n')\n",
    "print('Simplified lesk definition:', simplified_lesk_sense.definition(), sep='\\n')\n",
    "\n",
    "original_lesk_sense = original_lesk(sentence, 'school', 'n')\n",
    "print('Original lesk definition:', original_lesk_sense[1].definition(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extended Lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun, verb\n",
    "def hypernyms(synset):\n",
    "    return synset.hypernyms()\n",
    "\n",
    "def hyponyms(synset):\n",
    "    return synset.hyponyms()\n",
    "\n",
    "# noun\n",
    "def meronyms(synset):\n",
    "    return synset.substance_meronyms() + synset.part_meronyms() + synset.member_meronyms()\n",
    "\n",
    "def holonyms(synset):\n",
    "    return synset.substance_holonyms() + synset.part_holonyms() + synset.member_holonyms()\n",
    "\n",
    "# noun, adjective\n",
    "def attributes(synset):\n",
    "    return synset.attributes()\n",
    "\n",
    "# adjective\n",
    "def similar_tos(synset):\n",
    "    return synset.similar_tos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_glosses(synsets, func):\n",
    "    return [feature.definition() for synset in synsets for feature in func(synset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_gloss_score(synset1: str, synset2: str):\n",
    "    gloss1 = process_gloss(synset1)\n",
    "    gloss2 = process_gloss(synset2)\n",
    "\n",
    "    n1 = len(gloss1)\n",
    "    n2 = len(gloss2)\n",
    "    score = 0\n",
    "\n",
    "    for i in range(n1):\n",
    "        max_len = 0\n",
    "\n",
    "        for j in range(n2):\n",
    "            k = 0\n",
    "            while i + k < n1 and j + k < n2 and gloss1[i + k] == gloss2[j + k]:\n",
    "                k += 1\n",
    "\n",
    "            if k > max_len:\n",
    "                max_len = k\n",
    "\n",
    "        score += max_len ^ 2\n",
    "        i += max(0, max_len - 1)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_complex_score(target_gloss: str, synsets: [], feature_func: []):\n",
    "    score = 0\n",
    "\n",
    "    glosses = [synset.definition() for synset in synsets]\n",
    "    score += best_score_glosses(target_gloss, glosses, complex_gloss_score)\n",
    "\n",
    "    for func in feature_func:\n",
    "        glosses = feature_glosses(synsets, func)\n",
    "        score += best_score_glosses(target_gloss, glosses, complex_gloss_score)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_lesk(sentence: str, word: str, pos: str, feature_func: []):\n",
    "    target_synsets = wordnet.synsets(word, pos)\n",
    "    sentence_processed = process_sentence(sentence)\n",
    "\n",
    "    best_score = (-1, '')\n",
    "    for target_synset in target_synsets:\n",
    "        score = 0\n",
    "        \n",
    "        for synsets in sentence_processed:\n",
    "            score += compute_complex_score(target_synset.definition(), synsets, feature_func)\n",
    "\n",
    "        if score > best_score[0]:\n",
    "            best_score = (score, target_synset)\n",
    "\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the measure for each pair of synsets with five different sets of relations taken into acount in measuring the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "synset_list = get_synset_list(word_tokenize('Students enjoy going school studying reading books'))\n",
    "combinations = list(itertools.combinations(synset_list, 2))\n",
    "\n",
    "relations = [\n",
    "    [hypernyms, hyponyms, meronyms, holonyms, attributes, similar_tos],\n",
    "    [hypernyms, hyponyms, meronyms, holonyms],\n",
    "    [hyponyms, meronyms],\n",
    "    [hypernyms, holonyms],\n",
    "    [hyponyms, meronyms, similar_tos]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════════════════════╤═══════╤════════════════════════╤═════════════╤══════════════╤═════════════════════╕\n",
      "│ synsets                       │   all │   hyper+hypo+mero+holo │   hypo+mero │   hyper+holo │   hypo+mero+similar │\n",
      "╞═══════════════════════════════╪═══════╪════════════════════════╪═════════════╪══════════════╪═════════════════════╡\n",
      "│ student.n.01 - enjoy.v.01     │    16 │                     16 │          16 │            8 │                  16 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ student.n.01 - departure.n.01 │    24 │                     24 │          16 │           16 │                  16 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ student.n.01 - school.n.01    │    32 │                     32 │          23 │           16 │                  23 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ student.n.01 - perusal.n.01   │    16 │                     16 │           8 │           16 │                   8 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ student.n.01 - reading.n.01   │    24 │                     24 │          16 │           16 │                  16 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ student.n.01 - book.n.01      │    32 │                     32 │          24 │           16 │                  24 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ enjoy.v.01 - departure.n.01   │    42 │                     42 │          28 │           28 │                  28 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ enjoy.v.01 - school.n.01      │    56 │                     56 │          42 │           28 │                  42 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ enjoy.v.01 - perusal.n.01     │    28 │                     28 │          14 │           28 │                  14 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ enjoy.v.01 - reading.n.01     │    42 │                     42 │          28 │           28 │                  28 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ enjoy.v.01 - book.n.01        │    56 │                     56 │          42 │           28 │                  42 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ departure.n.01 - school.n.01  │    16 │                     16 │          12 │            8 │                  12 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ departure.n.01 - perusal.n.01 │     8 │                      8 │           4 │            8 │                   4 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ departure.n.01 - reading.n.01 │    12 │                     12 │           8 │            8 │                   8 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ departure.n.01 - book.n.01    │    16 │                     16 │          12 │            8 │                  12 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ school.n.01 - perusal.n.01    │     8 │                      8 │           4 │            8 │                   4 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ school.n.01 - reading.n.01    │    12 │                     12 │           8 │            8 │                   8 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ school.n.01 - book.n.01       │    16 │                     16 │          12 │            8 │                  12 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ perusal.n.01 - reading.n.01   │    26 │                     26 │          18 │           16 │                  18 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ perusal.n.01 - book.n.01      │    32 │                     32 │          24 │           16 │                  24 │\n",
      "├───────────────────────────────┼───────┼────────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ reading.n.01 - book.n.01      │    50 │                     50 │          38 │           25 │                  38 │\n",
      "╘═══════════════════════════════╧═══════╧════════════════════════╧═════════════╧══════════════╧═════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print_table = []\n",
    "for synsets in combinations:\n",
    "    relation_scores = [synsets[0].name() + ' - ' + synsets[1].name()]\n",
    "\n",
    "    for relation in relations:\n",
    "        relation_scores.append(compute_complex_score(synsets[0].definition(), [synsets[1]], relation))\n",
    "\n",
    "    print_table.append(relation_scores)\n",
    "\n",
    "print(tabulate(print_table, \n",
    "               headers=['synsets', 'all', 'hyper+hypo+mero+holo', 'hypo+mero', 'hyper+holo', 'hypo+mero+similar'], \n",
    "               tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the word sense for the given text and word and print its definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended lesk definition:\n",
      "a body of creative artists or writers or thinkers linked by a similar style or by similar teachers\n"
     ]
    }
   ],
   "source": [
    "feature_func = [hypernyms, hyponyms, meronyms, holonyms, attributes, similar_tos]\n",
    "\n",
    "extended_lesk_sense = extended_lesk(sentence, 'school', 'n', feature_func)\n",
    "print('Extended lesk definition:', extended_lesk_sense[1].definition(), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
