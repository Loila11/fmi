{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS & Syntax assignment part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\mirun\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from wikipedia) (2.22.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from wikipedia) (4.8.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\mirun\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (1.9.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\mirun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\mirun\\anaconda3\\lib\\site-packages (0.8.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\mirun\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Subsection 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the article:\n",
      "Natural language processing\n",
      "\n",
      "First 200 words:\n",
      "['Natural', 'language', 'processing', '(NLP)', 'is', 'a', 'subfield', 'of', 'linguistics,', 'computer', 'science,', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language,', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '\"understanding\"', 'the', 'contents', 'of', 'documents,', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition,', 'natural', 'language', 'understanding,', 'and', 'natural', 'language', 'generation.', '==', 'History', '==', 'Natural', 'language', 'processing', 'has', 'its', 'roots', 'in', 'the', '1950s.', 'Already', 'in', '1950,', 'Alan', 'Turing', 'published', 'an', 'article', 'titled', '\"Computing', 'Machinery', 'and', 'Intelligence\"', 'which', 'proposed', 'what', 'is', 'now', 'called', 'the', 'Turing', 'test', 'as', 'a', 'criterion', 'of', 'intelligence,', 'though', 'at', 'the', 'time', 'that', 'was', 'not', 'articulated', 'as', 'a', 'problem', 'separate', 'from', 'artificial', 'intelligence.', 'The', 'proposed', 'test', 'includes', 'a', 'task', 'that', 'involves', 'the', 'automated', 'interpretation', 'and', 'generation', 'of', 'natural', 'language.', '===', 'Symbolic', 'NLP', '(1950s', '–', 'early', '1990s)', '===', 'The', 'premise', 'of', 'symbolic', 'NLP', 'is', 'well-summarized', 'by', 'John', \"Searle's\", 'Chinese', 'room', 'experiment:', 'Given', 'a', 'collection', 'of', 'rules', '(e.g.,', 'a', 'Chinese', 'phrasebook,', 'with', 'questions']\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "nlp = wikipedia.page(\"Natural language processing\")\n",
    "\n",
    "print('Title of the article:')\n",
    "print(nlp.title)\n",
    "\n",
    "print('\\nFirst 200 words:')\n",
    "print([word for word in nlp.content.split()[:200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk.data\n",
    "import string\n",
    "\n",
    "def posTagSentences(sentence_list):\n",
    "    sentences_to_words = [nltk.wordpunct_tokenize(sentence) for sentence in sentence_list]\n",
    "    sentences_with_punct = [nltk.pos_tag(sentence) for sentence in sentences_to_words]\n",
    "    sentences = [[word for word in sentence if word[0][0] not in string.punctuation] for sentence in sentences_with_punct]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging for the first 20 sentences:\n",
      "\n",
      "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('NLP', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('linguistics', 'NNS'), ('computer', 'NN'), ('science', 'NN'), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('language', 'NN'), ('in', 'IN'), ('particular', 'JJ'), ('how', 'WRB'), ('to', 'TO'), ('program', 'NN'), ('computers', 'NNS'), ('to', 'TO'), ('process', 'VB'), ('and', 'CC'), ('analyze', 'VB'), ('large', 'JJ'), ('amounts', 'NNS'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS')]\n",
      "\n",
      "[('The', 'DT'), ('goal', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('computer', 'NN'), ('capable', 'NN'), ('of', 'IN'), ('understanding', 'VBG'), ('the', 'DT'), ('contents', 'NNS'), ('of', 'IN'), ('documents', 'NNS'), ('including', 'VBG'), ('the', 'DT'), ('contextual', 'JJ'), ('nuances', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('language', 'NN'), ('within', 'IN'), ('them', 'PRP')]\n",
      "\n",
      "[('The', 'DT'), ('technology', 'NN'), ('can', 'MD'), ('then', 'RB'), ('accurately', 'RB'), ('extract', 'JJ'), ('information', 'NN'), ('and', 'CC'), ('insights', 'NNS'), ('contained', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('documents', 'NNS'), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('categorize', 'NN'), ('and', 'CC'), ('organize', 'VB'), ('the', 'DT'), ('documents', 'NNS'), ('themselves', 'PRP')]\n",
      "\n",
      "[('Challenges', 'NNS'), ('in', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('frequently', 'RB'), ('involve', 'VBP'), ('speech', 'NN'), ('recognition', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('understanding', 'NN'), ('and', 'CC'), ('natural', 'JJ'), ('language', 'NN'), ('generation', 'NN')]\n",
      "\n",
      "[('History', 'NNP'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'NN'), ('has', 'VBZ'), ('its', 'PRP$'), ('roots', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('1950s', 'CD')]\n",
      "\n",
      "[('Already', 'RB'), ('in', 'IN'), ('1950', 'CD'), ('Alan', 'NNP'), ('Turing', 'NNP'), ('published', 'VBD'), ('an', 'DT'), ('article', 'NN'), ('titled', 'VBN'), ('Computing', 'VBG'), ('Machinery', 'NN'), ('and', 'CC'), ('Intelligence', 'NNP'), ('which', 'WDT'), ('proposed', 'VBD'), ('what', 'WP'), ('is', 'VBZ'), ('now', 'RB'), ('called', 'VBN'), ('the', 'DT'), ('Turing', 'NNP'), ('test', 'NN'), ('as', 'IN'), ('a', 'DT'), ('criterion', 'NN'), ('of', 'IN'), ('intelligence', 'NN'), ('though', 'RB'), ('at', 'IN'), ('the', 'DT'), ('time', 'NN'), ('that', 'WDT'), ('was', 'VBD'), ('not', 'RB'), ('articulated', 'VBN'), ('as', 'IN'), ('a', 'DT'), ('problem', 'NN'), ('separate', 'NN'), ('from', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN')]\n",
      "\n",
      "[('The', 'DT'), ('proposed', 'VBN'), ('test', 'NN'), ('includes', 'VBZ'), ('a', 'DT'), ('task', 'NN'), ('that', 'WDT'), ('involves', 'VBZ'), ('the', 'DT'), ('automated', 'JJ'), ('interpretation', 'NN'), ('and', 'CC'), ('generation', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN')]\n",
      "\n",
      "[('Symbolic', 'NNP'), ('NLP', 'NNP'), ('1950s', 'CD'), ('–', 'RB'), ('early', 'RB'), ('1990s', 'CD'), ('The', 'DT'), ('premise', 'NN'), ('of', 'IN'), ('symbolic', 'JJ'), ('NLP', 'NNP'), ('is', 'VBZ'), ('well', 'RB'), ('summarized', 'VBN'), ('by', 'IN'), ('John', 'NNP'), ('Searle', 'NNP'), ('s', 'NN'), ('Chinese', 'JJ'), ('room', 'NN'), ('experiment', 'NN'), ('Given', 'VBN'), ('a', 'DT'), ('collection', 'NN'), ('of', 'IN'), ('rules', 'NNS'), ('e', 'NN'), ('g', 'NN'), ('a', 'DT'), ('Chinese', 'JJ'), ('phrasebook', 'NN'), ('with', 'IN'), ('questions', 'NNS'), ('and', 'CC'), ('matching', 'VBG'), ('answers', 'NNS'), ('the', 'DT'), ('computer', 'NN'), ('emulates', 'VBZ'), ('natural', 'JJ'), ('language', 'NN'), ('understanding', 'NN'), ('or', 'CC'), ('other', 'JJ'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('by', 'IN'), ('applying', 'VBG'), ('those', 'DT'), ('rules', 'NNS'), ('to', 'TO'), ('the', 'DT'), ('data', 'NNS'), ('it', 'PRP'), ('confronts', 'VBZ')]\n",
      "\n",
      "[('1950s', 'NNS'), ('The', 'DT'), ('Georgetown', 'NNP'), ('experiment', 'NN'), ('in', 'IN'), ('1954', 'CD'), ('involved', 'JJ'), ('fully', 'RB'), ('automatic', 'JJ'), ('translation', 'NN'), ('of', 'IN'), ('more', 'JJR'), ('than', 'IN'), ('sixty', 'NN'), ('Russian', 'JJ'), ('sentences', 'NNS'), ('into', 'IN'), ('English', 'NNP')]\n",
      "\n",
      "[('The', 'DT'), ('authors', 'NNS'), ('claimed', 'VBD'), ('that', 'IN'), ('within', 'IN'), ('three', 'CD'), ('or', 'CC'), ('five', 'CD'), ('years', 'NNS'), ('machine', 'NN'), ('translation', 'NN'), ('would', 'MD'), ('be', 'VB'), ('a', 'DT'), ('solved', 'JJ'), ('problem', 'NN')]\n",
      "\n",
      "[('However', 'RB'), ('real', 'JJ'), ('progress', 'NN'), ('was', 'VBD'), ('much', 'RB'), ('slower', 'JJR'), ('and', 'CC'), ('after', 'IN'), ('the', 'DT'), ('ALPAC', 'NNP'), ('report', 'NN'), ('in', 'IN'), ('1966', 'CD'), ('which', 'WDT'), ('found', 'VBD'), ('that', 'IN'), ('ten', 'SYM'), ('year', 'NN'), ('long', 'JJ'), ('research', 'NN'), ('had', 'VBD'), ('failed', 'VBN'), ('to', 'TO'), ('fulfill', 'VB'), ('the', 'DT'), ('expectations', 'NNS'), ('funding', 'VBG'), ('for', 'IN'), ('machine', 'NN'), ('translation', 'NN'), ('was', 'VBD'), ('dramatically', 'RB'), ('reduced', 'VBN')]\n",
      "\n",
      "[('Little', 'JJ'), ('further', 'JJ'), ('research', 'NN'), ('in', 'IN'), ('machine', 'NN'), ('translation', 'NN'), ('was', 'VBD'), ('conducted', 'VBN'), ('until', 'IN'), ('the', 'DT'), ('late', 'JJ'), ('1980s', 'NNS'), ('when', 'WRB'), ('the', 'DT'), ('first', 'JJ'), ('statistical', 'JJ'), ('machine', 'NN'), ('translation', 'NN'), ('systems', 'NNS'), ('were', 'VBD'), ('developed', 'VBN')]\n",
      "\n",
      "[('1960s', 'NNS'), ('Some', 'DT'), ('notably', 'RB'), ('successful', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'VBG'), ('systems', 'NNS'), ('developed', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('1960s', 'NNS'), ('were', 'VBD'), ('SHRDLU', 'NNP'), ('a', 'DT'), ('natural', 'JJ'), ('language', 'NN'), ('system', 'NN'), ('working', 'VBG'), ('in', 'IN'), ('restricted', 'JJ'), ('blocks', 'NNS'), ('worlds', 'NNS'), ('with', 'IN'), ('restricted', 'JJ'), ('vocabularies', 'NNS'), ('and', 'CC'), ('ELIZA', 'NNP'), ('a', 'DT'), ('simulation', 'NN'), ('of', 'IN'), ('a', 'DT'), ('Rogerian', 'JJ'), ('psychotherapist', 'NN'), ('written', 'VBN'), ('by', 'IN'), ('Joseph', 'NNP'), ('Weizenbaum', 'NNP'), ('between', 'IN'), ('1964', 'CD'), ('and', 'CC'), ('1966', 'CD')]\n",
      "\n",
      "[('Using', 'VBG'), ('almost', 'RB'), ('no', 'DT'), ('information', 'NN'), ('about', 'IN'), ('human', 'JJ'), ('thought', 'NN'), ('or', 'CC'), ('emotion', 'NN'), ('ELIZA', 'NNP'), ('sometimes', 'RB'), ('provided', 'VBD'), ('a', 'DT'), ('startlingly', 'RB'), ('human', 'JJ'), ('like', 'IN'), ('interaction', 'NN')]\n",
      "\n",
      "[('When', 'WRB'), ('the', 'DT'), ('patient', 'NN'), ('exceeded', 'VBD'), ('the', 'DT'), ('very', 'RB'), ('small', 'JJ'), ('knowledge', 'NN'), ('base', 'NN'), ('ELIZA', 'NNP'), ('might', 'MD'), ('provide', 'VB'), ('a', 'DT'), ('generic', 'JJ'), ('response', 'NN'), ('for', 'IN'), ('example', 'NN'), ('responding', 'VBG'), ('to', 'TO'), ('My', 'NNP'), ('head', 'NN'), ('hurts', 'VBZ'), ('with', 'IN'), ('Why', 'WRB'), ('do', 'VBP'), ('you', 'PRP'), ('say', 'VB'), ('your', 'PRP$'), ('head', 'NN'), ('hurts', 'VBZ')]\n",
      "\n",
      "[('1970s', 'NNS'), ('During', 'IN'), ('the', 'DT'), ('1970s', 'CD'), ('many', 'JJ'), ('programmers', 'NNS'), ('began', 'VBD'), ('to', 'TO'), ('write', 'VB'), ('conceptual', 'JJ'), ('ontologies', 'NNS'), ('which', 'WDT'), ('structured', 'VBD'), ('real', 'JJ'), ('world', 'NN'), ('information', 'NN'), ('into', 'IN'), ('computer', 'NN'), ('understandable', 'JJ'), ('data', 'NNS')]\n",
      "\n",
      "[('Examples', 'NNS'), ('are', 'VBP'), ('MARGIE', 'NNP'), ('Schank', 'NNP'), ('1975', 'CD'), ('SAM', 'NNP'), ('Cullingford', 'NNP'), ('1978', 'CD'), ('PAM', 'NNP'), ('Wilensky', 'NNP'), ('1978', 'CD'), ('TaleSpin', 'NNP'), ('Meehan', 'NNP'), ('1976', 'CD'), ('QUALM', 'NNP'), ('Lehnert', 'NNP'), ('1977', 'CD'), ('Politics', 'NNP'), ('Carbonell', 'NNP'), ('1979', 'CD'), ('and', 'CC'), ('Plot', 'NNP'), ('Units', 'NNP'), ('Lehnert', 'NNP'), ('1981', 'CD')]\n",
      "\n",
      "[('During', 'IN'), ('this', 'DT'), ('time', 'NN'), ('the', 'DT'), ('first', 'JJ'), ('chatterbots', 'NNS'), ('were', 'VBD'), ('written', 'VBN'), ('e', 'NN'), ('g', 'NN'), ('PARRY', 'NNP')]\n",
      "\n",
      "[('1980s', 'NNS'), ('The', 'DT'), ('1980s', 'CD'), ('and', 'CC'), ('early', 'RB'), ('1990s', 'CD'), ('mark', 'NN'), ('the', 'DT'), ('hey', 'JJ'), ('day', 'NN'), ('of', 'IN'), ('symbolic', 'JJ'), ('methods', 'NNS'), ('in', 'IN'), ('NLP', 'NNP')]\n",
      "\n",
      "[('Focus', 'NNP'), ('areas', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), ('included', 'VBD'), ('research', 'NN'), ('on', 'IN'), ('rule', 'NN'), ('based', 'VBN'), ('parsing', 'NN'), ('e', 'NN'), ('g', 'NN'), ('the', 'DT'), ('development', 'NN'), ('of', 'IN'), ('HPSG', 'NNP'), ('as', 'IN'), ('a', 'DT'), ('computational', 'JJ'), ('operationalization', 'NN'), ('of', 'IN'), ('generative', 'JJ'), ('grammar', 'NN'), ('morphology', 'NN'), ('e', 'NN'), ('g', 'JJ'), ('two', 'CD'), ('level', 'NN'), ('morphology', 'NN'), ('semantics', 'NNS'), ('e', 'NN'), ('g', 'NN'), ('Lesk', 'NNP'), ('algorithm', 'NN'), ('reference', 'NN'), ('e', 'NN'), ('g', 'NN'), ('within', 'IN'), ('Centering', 'NNP'), ('Theory', 'NNP'), ('and', 'CC'), ('other', 'JJ'), ('areas', 'NNS'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('understanding', 'NN'), ('e', 'NN'), ('g', 'NN'), ('in', 'IN'), ('the', 'DT'), ('Rhetorical', 'JJ'), ('Structure', 'NNP'), ('Theory', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "sentences_from_wiki = nltk.tokenize.sent_tokenize(nlp.content)[:20]\n",
    "sentences = posTagSentences(sentences_from_wiki)\n",
    "\n",
    "print('POS tagging for the first 20 sentences:')\n",
    "for sentence in sentences:\n",
    "    print()\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Subsection 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language',\n",
       " 'processing',\n",
       " 'subfield',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'intelligence',\n",
       " 'language',\n",
       " 'program',\n",
       " 'language',\n",
       " 'goal',\n",
       " 'computer',\n",
       " 'capable',\n",
       " 'language',\n",
       " 'technology',\n",
       " 'information',\n",
       " 'categorize',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'language',\n",
       " 'generation',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'article',\n",
       " 'Machinery',\n",
       " 'test',\n",
       " 'criterion',\n",
       " 'intelligence',\n",
       " 'time',\n",
       " 'problem',\n",
       " 'separate',\n",
       " 'intelligence',\n",
       " 'test',\n",
       " 'task',\n",
       " 'interpretation',\n",
       " 'generation',\n",
       " 'language',\n",
       " 'premise',\n",
       " 's',\n",
       " 'room',\n",
       " 'experiment',\n",
       " 'collection',\n",
       " 'e',\n",
       " 'g',\n",
       " 'phrasebook',\n",
       " 'computer',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'experiment',\n",
       " 'translation',\n",
       " 'sixty',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'problem',\n",
       " 'progress',\n",
       " 'report',\n",
       " 'year',\n",
       " 'research',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'research',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'language',\n",
       " 'language',\n",
       " 'system',\n",
       " 'simulation',\n",
       " 'psychotherapist',\n",
       " 'information',\n",
       " 'thought',\n",
       " 'emotion',\n",
       " 'interaction',\n",
       " 'patient',\n",
       " 'knowledge',\n",
       " 'base',\n",
       " 'response',\n",
       " 'example',\n",
       " 'head',\n",
       " 'head',\n",
       " 'world',\n",
       " 'information',\n",
       " 'computer',\n",
       " 'time',\n",
       " 'e',\n",
       " 'g',\n",
       " 'mark',\n",
       " 'day',\n",
       " 'time',\n",
       " 'research',\n",
       " 'rule',\n",
       " 'parsing',\n",
       " 'e',\n",
       " 'g',\n",
       " 'development',\n",
       " 'operationalization',\n",
       " 'grammar',\n",
       " 'morphology',\n",
       " 'e',\n",
       " 'level',\n",
       " 'morphology',\n",
       " 'e',\n",
       " 'g',\n",
       " 'algorithm',\n",
       " 'reference',\n",
       " 'e',\n",
       " 'g',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'e',\n",
       " 'g']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPOS(pos, sentences):\n",
    "    return [word[0] for sentence in sentences for word in sentence if word[1] == pos]\n",
    "\n",
    "getPOS('NN', sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language',\n",
       " 'processing',\n",
       " 'subfield',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'intelligence',\n",
       " 'language',\n",
       " 'program',\n",
       " 'language',\n",
       " 'goal',\n",
       " 'computer',\n",
       " 'capable',\n",
       " 'language',\n",
       " 'technology',\n",
       " 'information',\n",
       " 'categorize',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'language',\n",
       " 'generation',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'article',\n",
       " 'Machinery',\n",
       " 'test',\n",
       " 'criterion',\n",
       " 'intelligence',\n",
       " 'time',\n",
       " 'problem',\n",
       " 'separate',\n",
       " 'intelligence',\n",
       " 'test',\n",
       " 'task',\n",
       " 'interpretation',\n",
       " 'generation',\n",
       " 'language',\n",
       " 'premise',\n",
       " 's',\n",
       " 'room',\n",
       " 'experiment',\n",
       " 'collection',\n",
       " 'e',\n",
       " 'g',\n",
       " 'phrasebook',\n",
       " 'computer',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'experiment',\n",
       " 'translation',\n",
       " 'sixty',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'problem',\n",
       " 'progress',\n",
       " 'report',\n",
       " 'year',\n",
       " 'research',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'research',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'language',\n",
       " 'language',\n",
       " 'system',\n",
       " 'simulation',\n",
       " 'psychotherapist',\n",
       " 'information',\n",
       " 'thought',\n",
       " 'emotion',\n",
       " 'interaction',\n",
       " 'patient',\n",
       " 'knowledge',\n",
       " 'base',\n",
       " 'response',\n",
       " 'example',\n",
       " 'head',\n",
       " 'head',\n",
       " 'world',\n",
       " 'information',\n",
       " 'computer',\n",
       " 'time',\n",
       " 'e',\n",
       " 'g',\n",
       " 'mark',\n",
       " 'day',\n",
       " 'time',\n",
       " 'research',\n",
       " 'rule',\n",
       " 'parsing',\n",
       " 'e',\n",
       " 'g',\n",
       " 'development',\n",
       " 'operationalization',\n",
       " 'grammar',\n",
       " 'morphology',\n",
       " 'e',\n",
       " 'level',\n",
       " 'morphology',\n",
       " 'e',\n",
       " 'g',\n",
       " 'algorithm',\n",
       " 'reference',\n",
       " 'e',\n",
       " 'g',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'e',\n",
       " 'g',\n",
       " 'a',\n",
       " 'the',\n",
       " 'The',\n",
       " 'a',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'The',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'an',\n",
       " 'the',\n",
       " 'a',\n",
       " 'the',\n",
       " 'a',\n",
       " 'The',\n",
       " 'a',\n",
       " 'the',\n",
       " 'The',\n",
       " 'a',\n",
       " 'a',\n",
       " 'the',\n",
       " 'those',\n",
       " 'the',\n",
       " 'The',\n",
       " 'The',\n",
       " 'a',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'Some',\n",
       " 'the',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'no',\n",
       " 'a',\n",
       " 'the',\n",
       " 'the',\n",
       " 'a',\n",
       " 'the',\n",
       " 'this',\n",
       " 'the',\n",
       " 'The',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'a',\n",
       " 'the']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPOSList(posList, sentences):\n",
    "    return [word for pos in posList for word in getPOS(pos, sentences)]\n",
    "\n",
    "getPOSList(['NN', 'DT'], sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Subsection 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'processing', 'subfield', 'computer', 'science', 'intelligence', 'language', 'program', 'language', 'goal', 'computer', 'capable', 'language', 'technology', 'information', 'categorize', 'language', 'processing', 'speech', 'recognition', 'language', 'understanding', 'language', 'generation', 'language', 'processing', 'article', 'Machinery', 'test', 'criterion', 'intelligence', 'time', 'problem', 'separate', 'intelligence', 'test', 'task', 'interpretation', 'generation', 'language', 'premise', 's', 'room', 'experiment', 'collection', 'e', 'g', 'phrasebook', 'computer', 'language', 'understanding', 'experiment', 'translation', 'sixty', 'machine', 'translation', 'problem', 'progress', 'report', 'year', 'research', 'machine', 'translation', 'research', 'machine', 'translation', 'machine', 'translation', 'language', 'language', 'system', 'simulation', 'psychotherapist', 'information', 'thought', 'emotion', 'interaction', 'patient', 'knowledge', 'base', 'response', 'example', 'head', 'head', 'world', 'information', 'computer', 'time', 'e', 'g', 'mark', 'day', 'time', 'research', 'rule', 'parsing', 'e', 'g', 'development', 'operationalization', 'grammar', 'morphology', 'e', 'level', 'morphology', 'e', 'g', 'algorithm', 'reference', 'e', 'g', 'language', 'understanding', 'e', 'g', 'linguistics', 'interactions', 'computers', 'computers', 'amounts', 'data', 'contents', 'documents', 'nuances', 'insights', 'documents', 'documents', 'Challenges', 'roots', 'rules', 'questions', 'answers', 'tasks', 'rules', 'data', '1950s', 'sentences', 'authors', 'years', 'expectations', '1980s', 'systems', '1960s', 'systems', '1960s', 'blocks', 'worlds', 'vocabularies', '1970s', 'programmers', 'ontologies', 'data', 'Examples', 'chatterbots', '1980s', 'methods', 'areas', 'semantics', 'areas', 'NLP', 'History', 'Natural', 'Alan', 'Turing', 'Intelligence', 'Turing', 'Symbolic', 'NLP', 'NLP', 'John', 'Searle', 'NLP', 'Georgetown', 'English', 'ALPAC', 'SHRDLU', 'ELIZA', 'Joseph', 'Weizenbaum', 'ELIZA', 'ELIZA', 'My', 'MARGIE', 'Schank', 'SAM', 'Cullingford', 'PAM', 'Wilensky', 'TaleSpin', 'Meehan', 'QUALM', 'Lehnert', 'Politics', 'Carbonell', 'Plot', 'Units', 'Lehnert', 'PARRY', 'NLP', 'Focus', 'HPSG', 'Lesk', 'Centering', 'Theory', 'Structure', 'Theory', 'process', 'analyze', 'organize', 'be', 'fulfill', 'provide', 'say', 'write', 'understanding', 'including', 'Computing', 'matching', 'applying', 'funding', 'processing', 'working', 'Using', 'responding', 'published', 'proposed', 'was', 'claimed', 'was', 'found', 'had', 'was', 'was', 'were', 'were', 'provided', 'exceeded', 'began', 'structured', 'were', 'included', 'concerned', 'contained', 'titled', 'called', 'articulated', 'proposed', 'summarized', 'Given', 'failed', 'reduced', 'conducted', 'developed', 'developed', 'written', 'written', 'based', 'involve', 'do', 'are', 'is', 'is', 'has', 'is', 'includes', 'involves', 'is', 'emulates', 'confronts', 'hurts', 'hurts']\n"
     ]
    }
   ],
   "source": [
    "content_words = getPOSList(['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ'], sentences)\n",
    "print(content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of content words from the text:\n",
      "0.517175572519084\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of content words from the text:')\n",
    "print(len(content_words) / sum([len(sentence) for sentence in sentences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Subsection 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagmap(pos):\n",
    "    if pos[0] == 'J':\n",
    "        return 'a'\n",
    "    elif pos[0] == 'V':\n",
    "        return 'v'\n",
    "    elif pos[0] == 'R':\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════╤═══════╤════════════════════════╤══════════════════════════╕\n",
      "│ Original word   │ POS   │ Simple lemmatization   │ Lemmatization with POS   │\n",
      "╞═════════════════╪═══════╪════════════════════════╪══════════════════════════╡\n",
      "│ including       │ VBG   │ including              │ include                  │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ concerned       │ VBN   │ concerned              │ concern                  │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ emulates        │ VBZ   │ emulates               │ emulate                  │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ titled          │ VBN   │ titled                 │ title                    │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ summarized      │ VBN   │ summarized             │ summarize                │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ is              │ VBZ   │ is                     │ be                       │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ includes        │ VBZ   │ includes               │ include                  │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ claimed         │ VBD   │ claimed                │ claim                    │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ articulated     │ VBN   │ articulated            │ articulate               │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ involves        │ VBZ   │ involves               │ involve                  │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ confronts       │ VBZ   │ confronts              │ confront                 │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ understanding   │ VBG   │ understanding          │ understand               │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ proposed        │ VBN   │ proposed               │ propose                  │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ applying        │ VBG   │ applying               │ apply                    │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ has             │ VBZ   │ ha                     │ have                     │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ contained       │ VBN   │ contained              │ contain                  │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ matching        │ VBG   │ matching               │ match                    │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ proposed        │ VBD   │ proposed               │ propose                  │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ called          │ VBN   │ called                 │ call                     │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ as              │ RB    │ a                      │ as                       │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ was             │ VBD   │ wa                     │ be                       │\n",
      "├─────────────────┼───────┼────────────────────────┼──────────────────────────┤\n",
      "│ published       │ VBD   │ published              │ publish                  │\n",
      "╘═════════════════╧═══════╧════════════════════════╧══════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from tabulate import tabulate\n",
    "\n",
    "lem=WordNetLemmatizer()\n",
    "\n",
    "def pos_table(sentences, N):\n",
    "    table = {'Original word': [], 'POS': [], 'Simple lemmatization': [], 'Lemmatization with POS': []}\n",
    "\n",
    "    word_set = set([word for sentence in sentences[:N] for word in sentence])\n",
    "\n",
    "    for word in word_set:\n",
    "        lemmatization1 = lem.lemmatize(word[0])\n",
    "        lemmatization2 = lem.lemmatize(word[0], pos=tagmap(word[1]))\n",
    "\n",
    "        if (lemmatization1 != lemmatization2):        \n",
    "            table['Original word'].append(word[0])\n",
    "            table['POS'].append(word[1])\n",
    "            table['Simple lemmatization'].append(lemmatization1)\n",
    "            table['Lemmatization with POS'].append(lemmatization2)\n",
    "\n",
    "    return tabulate(table, headers='keys', tablefmt='fancy_grid')\n",
    "\n",
    "print(pos_table(sentences, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Subsection 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CC', 18),\n",
       " ('CD', 21),\n",
       " ('DT', 51),\n",
       " ('IN', 60),\n",
       " ('JJ', 53),\n",
       " ('NN', 115),\n",
       " ('NNP', 47),\n",
       " ('NNS', 44),\n",
       " ('RB', 22),\n",
       " ('TO', 6),\n",
       " ('VB', 8),\n",
       " ('VBD', 17),\n",
       " ('VBG', 10),\n",
       " ('VBN', 16),\n",
       " ('VBZ', 11)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.data import load\n",
    "\n",
    "tag_dict = load('help/tagsets/upenn_tagset.pickle')\n",
    "pos_tags = tag_dict.keys()\n",
    "\n",
    "tag_len = [(tag, len(getPOS(tag, sentences))) for tag in pos_tags]\n",
    "tag_len = list(filter(lambda x: x[1] > 5, tag_len))\n",
    "tag_len.sort()\n",
    "tag_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel('Part of Speech')\n",
    "plt.ylabel('Number of Words')\n",
    "\n",
    "plt.bar(list(map(lambda x: x[0], tag_len)), list(map(lambda x: x[1], tag_len)))\n",
    "\n",
    "plt.savefig('POS_count.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Subsection 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_grammar = \"\"\"S -> NP VP\n",
    "S -> VP\n",
    "NP -> DT NN\n",
    "NP -> IN NNP\n",
    "NP -> DT JJ NN\n",
    "NP -> PRP\n",
    "VP -> VBP NP\n",
    "VP -> VBP VP\n",
    "VP -> VBG NP\n",
    "VP -> TO VP\n",
    "VP -> VB\n",
    "VP -> VB NP\n",
    "VP -> VBD\n",
    "VP -> VBD NP\n",
    "\"\"\"\n",
    "\n",
    "def make_grammar(raw_sentences):\n",
    "    sentences = posTagSentences(raw_sentences)\n",
    "    sentences = [(word[0].lower(), word[1]) for sentence in sentences for word in sentence]\n",
    "#     sentences = [(word[0].lower(), word[1][0]) if word[1][0] in 'N' else (word[0].lower(), word[1]) \n",
    "#                  for sentence in sentences for word in sentence]\n",
    "\n",
    "    sent = list(map(lambda x: x[0], sentences))\n",
    "\n",
    "    posList = list(set(map(lambda x: x[1], sentences)))\n",
    "\n",
    "    grammar = general_grammar\n",
    "\n",
    "    for pos in posList:\n",
    "        format_pos = set([word for word in getPOS(pos, [sentences])])\n",
    "        join_pos = '\"' + '\" | \"'.join(format_pos) + '\"'\n",
    "        grammar += f'{pos} -> {join_pos}\\n'\n",
    "\n",
    "    return sent, nltk.CFG.fromstring(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 19 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    S -> VP\n",
      "    NP -> DT NN\n",
      "    NP -> IN NNP\n",
      "    NP -> DT JJ NN\n",
      "    NP -> PRP\n",
      "    VP -> VBP NP\n",
      "    VP -> VBP VP\n",
      "    VP -> VBG NP\n",
      "    VP -> TO VP\n",
      "    VP -> VB\n",
      "    VP -> VB NP\n",
      "    VP -> VBD\n",
      "    VP -> VBD NP\n",
      "    VBP -> 'am'\n",
      "    NNP -> 'paul'\n",
      "    PRP -> 'i'\n",
      "    IN -> 'with'\n",
      "    VBG -> 'talking'\n"
     ]
    }
   ],
   "source": [
    "sent, grammar = make_grammar(['I am talking with Paul.'])\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rdp_tree(sent, grammar):\n",
    "    rdp = nltk.RecursiveDescentParser(grammar)\n",
    "\n",
    "    for tree in rdp.parse(sent):\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (PRP i))\n",
      "  (VP (VBP am) (VP (VBG talking) (NP (IN with) (NNP paul)))))\n"
     ]
    }
   ],
   "source": [
    "rdp_tree = get_rdp_tree(sent, grammar)\n",
    "print(rdp_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Subsection 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_srp_tree(sent, grammar):\n",
    "    srp = nltk.ShiftReduceParser(grammar)\n",
    "\n",
    "    for tree in srp.parse(sent):\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "srp_tree = get_srp_tree(sent, grammar)\n",
    "print(srp_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(rdp_tree == srp_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_trees(sentence):\n",
    "    sent, grammar = make_grammar(sentence)\n",
    "\n",
    "    rdp_tree = get_rdp_tree(sent, grammar)\n",
    "    print('Recursive Descent Parser Tree:')\n",
    "    print(rdp_tree)\n",
    "    print()\n",
    "\n",
    "    srp_tree = get_srp_tree(sent, grammar)\n",
    "    print('Shift Reduce Parser Tree:')\n",
    "    print(srp_tree)\n",
    "    print()\n",
    "\n",
    "    print('Are the trees equal?')\n",
    "    print(rdp_tree == srp_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive Descent Parser Tree:\n",
      "(S\n",
      "  (NP (PRP i))\n",
      "  (VP (VBP am) (VP (VBG talking) (NP (IN with) (NNP paul)))))\n",
      "\n",
      "Shift Reduce Parser Tree:\n",
      "None\n",
      "\n",
      "Are the trees equal?\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "compare_trees(['I am talking with Paul.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive Descent Parser Tree:\n",
      "(S (NP (PRP we)) (VP (VBD laughed)))\n",
      "\n",
      "Shift Reduce Parser Tree:\n",
      "(S (NP (PRP we)) (VP (VBD laughed)))\n",
      "\n",
      "Are the trees equal?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "compare_trees(['We laughed.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
